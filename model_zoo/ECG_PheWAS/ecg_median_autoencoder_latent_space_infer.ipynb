{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "reserved-yesterday",
   "metadata": {},
   "source": [
    "# ECG Latent Space Exploration\n",
    "This notebook shows how to create a latent space from a pretrained multimodal model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "atomic-insured",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "from typing import Callable, List, Dict\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import CategoricalCrossentropy\n",
    "\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from ml4h_ccds.data_descriptions.ecg import ECGDataDescription\n",
    "from ml4h_ccds.data_descriptions.wide_file import WideFileDataDescription\n",
    "from ml4h_ccds.data_descriptions.util import download_s3_if_not_exists\n",
    "\n",
    "from ml4h.models.model_factory import block_make_multimodal_multitask_model\n",
    "from ml4h.TensorMap import TensorMap, Interpretation\n",
    "\n",
    "# ml4h Imports\n",
    "from ml4h.arguments import parse_args\n",
    "from ml4h.metrics import coefficient_of_determination\n",
    "from ml4h.explorations import latent_space_dataframe\n",
    "from ml4h.models.model_factory import block_make_multimodal_multitask_model\n",
    "\n",
    "\n",
    "from ml4ht.data.util.date_selector import DateRangeOptionPicker, first_dt, DATE_OPTION_KEY, DateRangeOptionPicker\n",
    "from ml4ht.data.util.data_frame_data_description import DataFrameDataDescription \n",
    "from ml4ht.data.data_description import DataDescription\n",
    "from ml4ht.data.sample_getter import DataDescriptionSampleGetter\n",
    "from ml4ht.data.explore import explore_data_descriptions, explore_sample_getter\n",
    "from ml4ht.data.data_loader import SampleGetterDataset, numpy_collate_fn, SampleGetterIterableDataset\n",
    "\n",
    "import math\n",
    "import time\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from sklearn.linear_model import LogisticRegression, LinearRegression, ElasticNet, Ridge\n",
    "from sklearn.metrics import roc_curve, auc, precision_recall_curve, average_precision_score\n",
    "from sklearn.metrics import brier_score_loss, precision_score, recall_score, f1_score, roc_auc_score\n",
    "\n",
    "from ml4h.explorations import latent_space_dataframe\n",
    "\n",
    "# IPython imports\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import colors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "recorded-bahrain",
   "metadata": {},
   "outputs": [],
   "source": [
    "SESSION_DIR = os.path.expanduser(\"~\")  # downloaded data will be stored here"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "regulation-college",
   "metadata": {},
   "source": [
    "# ECG data description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "surgical-modification",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_ecg(ecg, _):\n",
    "    \"\"\"Transform ECG units to millivolts\"\"\"\n",
    "    return ecg / 1000\n",
    "\n",
    "def standardize_by_sample_ecg(ecg, _):\n",
    "    \"\"\"Transform ECG units to millivolts\"\"\"\n",
    "    return (ecg - np.mean(ecg)) / (np.std(ecg) + 1e-6) \n",
    "\n",
    "ecg_dd_i = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='input_ecg_5000_std_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[standardize_by_sample_ecg],  # these will be applied in order\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_mgh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "\n",
    "ecg_dd_o = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='output_ecg_5000_std_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[standardize_by_sample_ecg],  # these will be applied in order\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_mgh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "ecg_dd_i_lead_I = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='input_ecg_lead_I_5000_std_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[standardize_by_sample_ecg],  # these will be applied in order\n",
    "    leads={'I':0},\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_mgh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "\n",
    "ecg_dd_o_lead_I = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='output_ecg_lead_I_5000_std_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[standardize_by_sample_ecg],  # these will be applied in order\n",
    "    leads={'I':0},\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_mgh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "ecg_dd_i_lead_I_mv = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='input_ecg_lead_I_5000_mv_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[normalize_ecg],  # these will be applied in order\n",
    "    leads={'I':0},\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_mgh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "\n",
    "ecg_dd_o_lead_I_mv = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='output_ecg_lead_I_5000_mv_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[normalize_ecg],  # these will be applied in order\n",
    "    leads={'I':0},\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_mgh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22bba825",
   "metadata": {},
   "outputs": [],
   "source": [
    "ecg_dd_i_bwh = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='input_ecg_5000_std_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[standardize_by_sample_ecg],  # these will be applied in order\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_bwh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "ecg_dd_i_bwh_lead_I = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='input_ecg_lead_I_5000_std_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[standardize_by_sample_ecg],  # these will be applied in order\n",
    "    leads={'I':0},\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_bwh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")\n",
    "ecg_dd_i_bwh_lead_I_mv = ECGDataDescription(\n",
    "    SESSION_DIR, \n",
    "    name='input_ecg_lead_I_5000_mv_continuous', \n",
    "    ecg_len=5000,  # all ECGs will be linearly interpolated to be this length\n",
    "    transforms=[normalize_ecg],  # these will be applied in order\n",
    "    leads={'I':0},\n",
    "    # data will be automatically localized from s3\n",
    "    s3_bucket_name='2017P001650', s3_bucket_path=['ecg_bwh_hd5s'],   # 'ecg_mgh_hd5s',  list of hd5s\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85ce4414",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copy pasted from ml4h branch nd_ml4ht_integration\n",
    "def _not_implemented_tensor_from_file(_, __, ___=None):\n",
    "    \"\"\"Used to make sure TensorMap is never used to load data\"\"\"\n",
    "    raise NotImplementedError('This TensorMap cannot load data.')\n",
    "    \n",
    "\n",
    "def tensor_map_from_data_description(\n",
    "        data_description: DataDescription,\n",
    "        interpretation: Interpretation,\n",
    "        shape,\n",
    "        name=None,\n",
    "        **tensor_map_kwargs,\n",
    ") -> TensorMap:\n",
    "    \"\"\"\n",
    "    Allows a DataDescription to be used in the model factory\n",
    "    by converting a DataDescription into a TensorMap\n",
    "    \"\"\"\n",
    "    tmap = TensorMap(\n",
    "        name=name if name else data_description.name,\n",
    "        interpretation=interpretation,\n",
    "        shape=shape,\n",
    "        tensor_from_file=_not_implemented_tensor_from_file,\n",
    "        **tensor_map_kwargs,\n",
    "    )\n",
    "    return tmap\n",
    "\n",
    "\n",
    "ecg_tmap = tensor_map_from_data_description(\n",
    "    ecg_dd_i,\n",
    "    Interpretation.CONTINUOUS,\n",
    "    (5000, 12), name='ecg_5000_std'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fa6d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ml4h.arguments import parse_args\n",
    "from ml4h.metrics import coefficient_of_determination\n",
    "from ml4h.explorations import latent_space_dataframe\n",
    "from ml4h.models.model_factory import block_make_multimodal_multitask_model\n",
    "\n",
    "\n",
    "model_name = 'ecg_2500_autoencoder_mgh_c3po_128d_v2021_12_17'\n",
    "model_name = 'mgh_ecg_2500_std_autoencoder_v2022_03_29'\n",
    "sys.argv = ['train',\n",
    "            '--activation', 'mish',\n",
    "            '--block_size', '8', '--conv_width', '61', '--conv_layers', '64', '64', '--pool_type', 'max', \n",
    "            '--dense_blocks', '64', '64', '--dense_layers', '128', '--pool_type', 'average',\n",
    "            '--learning_rate', '0.00002',\n",
    "            '--encoder_blocks', 'conv_encode', '--merge_blocks', '--decoder_blocks', 'conv_decode',\n",
    "           '--tensormap_prefix', 'ml4h.tensormap.mgb.ecg',\n",
    "            '--model_file', f'../../ecg_rest_to_ecg_median_translator_256d.h5',\n",
    "           \n",
    "           ]\n",
    "args = parse_args()\n",
    "args.tensor_maps_in = [ecg_tmap]\n",
    "args.tensor_maps_out = [ecg_tmap]\n",
    "\n",
    "ecg_10s_2_median, _, _, _ = block_make_multimodal_multitask_model(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0750030",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ml4h.arguments import parse_args\n",
    "from ml4h.metrics import coefficient_of_determination\n",
    "from ml4h.explorations import latent_space_dataframe\n",
    "from ml4h.models.model_factory import block_make_multimodal_multitask_model\n",
    "\n",
    "model_name = 'mgh_ecg_rest_median_raw_10_autoencoder_256d_v2022_04_13'\n",
    "\n",
    "\n",
    "sys.argv = ['train',\n",
    "            '--activation', 'mish',\n",
    "            '--block_size', '8', '--conv_width', '61', '--conv_layers', '64', '64', '--pool_type', 'max', \n",
    "            '--dense_blocks', '64', '64', '--dense_layers', '128', '--pool_type', 'average',\n",
    "            '--learning_rate', '0.00002',\n",
    "            '--encoder_blocks', 'conv_encode', '--merge_blocks', '--decoder_blocks', 'conv_decode',\n",
    "           '--tensormap_prefix', 'ml4h.tensormap.mgb.ecg',\n",
    "            '--model_file', f'../../trained_models/{model_name}/{model_name}.h5',\n",
    "           \n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "\n",
    "ecg_tmap_median = tensor_map_from_data_description(\n",
    "    ecg_dd_i,\n",
    "    Interpretation.CONTINUOUS,\n",
    "    (600, 12), name='ecg_rest_median_raw_10'\n",
    ")\n",
    "args.tensor_maps_in = [ecg_tmap_median]\n",
    "args.tensor_maps_out = [ecg_tmap_median]\n",
    "\n",
    "ecg_autoencoder, encoders, decoders, merger = block_make_multimodal_multitask_model(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccf4cdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from ml4h.arguments import parse_args\n",
    "from ml4h.metrics import coefficient_of_determination\n",
    "from ml4h.explorations import latent_space_dataframe\n",
    "from ml4h.models.model_factory import block_make_multimodal_multitask_model\n",
    "\n",
    "model_name = 'mgh_ecg_rest_median_raw_10_lead_I_autoencoder_256d_v2022_04_09'\n",
    "\n",
    "\n",
    "sys.argv = ['train',\n",
    "            '--activation', 'mish',\n",
    "            '--block_size', '8', '--conv_width', '61', '--conv_layers', '64', '64', '--pool_type', 'max', \n",
    "            '--dense_blocks', '64', '64', '--dense_layers', '128', '--pool_type', 'average',\n",
    "            '--learning_rate', '0.00002',\n",
    "            '--encoder_blocks', 'conv_encode', '--merge_blocks', '--decoder_blocks', 'conv_decode',\n",
    "           '--tensormap_prefix', 'ml4h.tensormap.mgb.ecg',\n",
    "            '--model_file', f'../../trained_models/{model_name}/{model_name}.h5',\n",
    "           \n",
    "           ]\n",
    "args = parse_args()\n",
    "\n",
    "\n",
    "ecg_tmap_median_lead_I = tensor_map_from_data_description(\n",
    "    ecg_dd_i,\n",
    "    Interpretation.CONTINUOUS,\n",
    "    (600, 1), name='ecg_rest_median_raw_10_lead_I'\n",
    ")\n",
    "args.tensor_maps_in = [ecg_tmap_median_lead_I]\n",
    "args.tensor_maps_out = [ecg_tmap_median_lead_I]\n",
    "\n",
    "_, encoders_lead_I, _, _ = block_make_multimodal_multitask_model(**args.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40f0c968",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "mrn = 1519973\n",
    "mrn = 5212097\n",
    "mrn=4719681\n",
    "#mrn=4282470\n",
    "options = ecg_dd_i.get_loading_options(mrn)\n",
    "\n",
    "plt.plot(np.linspace(0, 10, 5000), ecg_dd_i.get_raw_data(mrn, options[-1]))\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()\n",
    "\n",
    "options = ecg_dd_i.get_loading_options(mrn)\n",
    "example = ecg_dd_i.get_raw_data(mrn, options[-1])\n",
    "example.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08602c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecg = ecg_10s_2_median(np.array([example]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d57c5569",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 10, 600), mecg[0])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e82cc5a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads = ['I', 'aVR', 'V1', 'V4', \n",
    "             'II', 'aVL', 'V2', 'V5', \n",
    "             'III', 'aVF', 'V3', 'V6', ]\n",
    "    \n",
    "channel_map = {\n",
    "    'I': 0, 'II': 1, 'III': 2, 'V1': 3, 'V2': 4, 'V3': 5,\n",
    "    'V4': 6, 'V5': 7, 'V6': 8, 'aVF': 9, 'aVL': 10, 'aVR': 11,\n",
    "}\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 4), dpi=300, sharey=False, sharex=True)\n",
    "for i, (ax, lead) in enumerate(zip(axes.ravel(), leads)):\n",
    "    ax.plot(range(600), mecg[0, :, channel_map[lead]])\n",
    "    ax.set_title(f\"Lead: {lead}\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"amplitude (mV)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45011e25",
   "metadata": {},
   "outputs": [],
   "source": [
    "leads = ['I', 'aVR', 'V1', 'V4', \n",
    "             'II', 'aVL', 'V2', 'V5', \n",
    "             'III', 'aVF', 'V3', 'V6', ]\n",
    "    \n",
    "channel_map = {\n",
    "    'I': 0, 'II': 1, 'III': 2, 'V1': 6, 'V2': 7, 'V3': 8,\n",
    "    'V4': 9, 'V5': 10, 'V6': 11, 'aVF': 5, 'aVL': 4, 'aVR': 3,\n",
    "}\n",
    "fig, axes = plt.subplots(3, 4, figsize=(12, 4), dpi=300, sharey=False, sharex=True)\n",
    "for i, (ax, lead) in enumerate(zip(axes.ravel(), leads)):\n",
    "    ax.plot(range(2000,3500), example[2000:3500, channel_map[lead]])\n",
    "    ax.set_title(f\"Lead: {lead}\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"amplitude (mV)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad4100da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import Counter, defaultdict\n",
    "from scipy import ndimage\n",
    "from biosppy.signals.ecg import ecg\n",
    "# Keras imports\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "\n",
    "# IPython imports\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56a7e0ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "def stretch_ecg(x, n=0):\n",
    "    \"\"\"\n",
    "    stretches input ECG to n bpm\n",
    "    \"\"\"\n",
    "    out = ecg(x.copy(), sampling_rate=500, show=False)\n",
    "    hr = out[-1].mean()\n",
    "    t = np.arange(len(x))\n",
    "    if n == 0:\n",
    "        tp = np.arange(len(x))\n",
    "    else:\n",
    "        tp = np.arange(len(x)) * n / hr\n",
    "    stretched = np.interp(tp, t, x)\n",
    "    print(f'{x.shape} and {stretched.shape} and hr: {hr} and n{n}')\n",
    "    out2 = ecg(stretched, show=False)\n",
    "    return stretched, out2[2]\n",
    "\n",
    "def plot_biosspy(mrn, bpm = 60):\n",
    "    options = ecg_dd_i.get_loading_options(mrn)\n",
    "    example = ecg_dd_i.get_raw_data(mrn, options[-1])\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(48, 16), dpi=300, sharey=False, sharex=True)\n",
    "    for i, (ax, lead) in enumerate(zip(axes.ravel(), leads)):\n",
    "        stretched, peaks = stretch_ecg(example[:,channel_map[lead]], bpm)\n",
    "        #print(f'at lead {lead} peaks are: {peaks}')\n",
    "        ax.plot(range(0,5000), stretched)\n",
    "        for p in peaks:\n",
    "            ax.axvline(p, linestyle='dashed', c='orange')\n",
    "        ax.set_title(f\"Lead: {lead}\")\n",
    "        ax.set_xlabel(\"time (s)\")\n",
    "        ax.set_ylabel(\"amplitude (mV)\")\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    \n",
    "    \n",
    "def plot_biosspy_median(mrn, median_size = 600, bpm = 0):\n",
    "    options = ecg_dd_i.get_loading_options(mrn)\n",
    "    example = ecg_dd_i.get_raw_data(mrn, options[-1])\n",
    "    \n",
    "    medians = np.zeros((median_size, len(channel_map)))\n",
    "    for i,lead in enumerate(leads):\n",
    "        waves = []\n",
    "        stretched, peaks = stretch_ecg(example[:,channel_map[lead]], bpm)\n",
    "        #print(f'at lead {lead} peaks are: {peaks}')\n",
    "\n",
    "        for j, p0 in enumerate(peaks[:-2]):\n",
    "            p11 = peaks[j+1]\n",
    "            p22 = peaks[j+2]\n",
    "            middle = (p0+p11)//2\n",
    "\n",
    "            #waves.append(np.interp(np.arange(median_size), np.arange(p22-middle), stretched[middle:p22]))\n",
    "            waves.append(stretched[middle:middle+median_size])\n",
    "        waves = np.array(waves)\n",
    "        #print(f'{waves.shape}')\n",
    "        medians[:, channel_map[lead]] = np.median(waves, axis=0)\n",
    "    fig, axes = plt.subplots(3, 4, figsize=(12, 4), dpi=300, sharey=False, sharex=True)\n",
    "    for i, (ax, lead) in enumerate(zip(axes.ravel(), leads)):\n",
    "        ax.plot(range(median_size), medians[:, channel_map[lead]])\n",
    "        ax.set_title(f\"Lead: {lead}\")\n",
    "        ax.set_xlabel(\"time (s)\")\n",
    "        ax.set_ylabel(\"amplitude (mV)\")    \n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9b7723",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biosspy_median(1519973, bpm = 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc1bb5ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_biosspy_median(4282470, bpm = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "485d1fac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#plot_biosspy_median(4282470, bpm=0)\n",
    "# plot_biosspy_median(4719681)\n",
    "# plot_biosspy_median(1519973)\n",
    "# plot_biosspy_median(4282470)\n",
    "plot_biosspy_median(1519973, bpm = 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b662abfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "mrn = 1519973\n",
    "#mrn = 5212097\n",
    "#mrn=4719681\n",
    "#plot_biosspy(5212097)\n",
    "# plot_biosspy(4719681)\n",
    "#plot_biosspy(1519973)\n",
    "\n",
    "plot_biosspy(1519973)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb71f5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "bpm = 60\n",
    "stretched, peaks = stretch_ecg(example[:,0], bpm)\n",
    "print(f'peaks are: {peaks}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607780d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(3, 4, figsize=(12, 4), dpi=300, sharey=False, sharex=True)\n",
    "for i, (ax, lead) in enumerate(zip(axes.ravel(), leads)):\n",
    "    stretched, peaks = stretch_ecg(example[:,channel_map[lead]], bpm)\n",
    "    print(f'at lead {lead} peaks are: {peaks}')\n",
    "    ax.plot(range(0,5000), stretched)\n",
    "    for p in peaks:\n",
    "        ax.axvline(p, linestyle='dashed', c='orange')\n",
    "    ax.set_title(f\"Lead: {lead}\")\n",
    "    ax.set_xlabel(\"time (s)\")\n",
    "    ax.set_ylabel(\"amplitude (mV)\")\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "closed-arrangement",
   "metadata": {},
   "outputs": [],
   "source": [
    "# download the wide file\n",
    "wide_path = download_s3_if_not_exists(\n",
    "    bucket_name='2017P001650',\n",
    "    bucket_path='csvs/charge_set_plusothers_2021_08_10_mgh_and_bwh.tsv',\n",
    "    local_dir=SESSION_DIR,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "sealed-whole",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the wide file\n",
    "wide_df = pd.read_csv(wide_path, sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "laughing-group",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.dropna(subset=[\"MRN\"], inplace=True)\n",
    "wide_df[\"MRN\"] = wide_df[\"MRN\"].astype(int)\n",
    "wide_df[\"fpath_x\"] = wide_df[\"fpath_x\"].astype(int, errors='ignore')\n",
    "wide_df[\"fpath_y\"] = wide_df[\"fpath_y\"].astype(int, errors='ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cosmetic-herald",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.set_index(\"MRN\", inplace=True)\n",
    "\n",
    "wide_df = wide_df[~wide_df.index.duplicated()]\n",
    "\n",
    "wide_df[\"af_age\"] = pd.to_timedelta(wide_df[\"af_age\"], unit=\"d\")\n",
    "\n",
    "wide_df[\"last_encounter\"] = pd.to_timedelta(wide_df[\"last_encounter\"],)\n",
    "wide_df[\"start_fu_date\"] = pd.to_datetime(wide_df[\"start_fu_date\"],)\n",
    "\n",
    "wide_df[\"start_fu_age\"] = pd.to_timedelta(wide_df[\"start_fu_age\"])\n",
    "wide_df[\"start_fu_age\"] = pd.to_timedelta(wide_df[\"start_fu_age\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33bdff79",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_hospital(row):\n",
    "    return 'BWH' if pd.isna(row['fpath_x']) else 'MGH'\n",
    "wide_df['hospital'] = wide_df.apply(lambda row: label_hospital(row), axis=1)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3729be14",
   "metadata": {},
   "outputs": [],
   "source": [
    "wide_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7275b957",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ColumnFromWideFileDD(DataDescription):\n",
    "    # DataDescription for a wide file\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        wide_df: pd.DataFrame,\n",
    "        column: str,  # e.g. Dem.Gender.no_filter\n",
    "        channel_map: Dict[str,int] = None,\n",
    "        transform: Callable = None,\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.wide_df = wide_df\n",
    "        self.column = column\n",
    "        self.channel_map = channel_map\n",
    "        self.transform = transform\n",
    "\n",
    "    def get_loading_options(self, sample_id):\n",
    "        return []\n",
    "\n",
    "    def get_raw_data(self, sample_id, loading_option):\n",
    "        row = self.wide_df.loc[[sample_id]].iloc[0]\n",
    "        value = row[self.column]\n",
    "        if self.channel_map:\n",
    "            tensor = np.zeros((len(self.channel_map),), dtype=np.float32)\n",
    "            for cm in self.channel_map:\n",
    "                if value.lower() == cm:\n",
    "                    tensor[self.channel_map[cm]] = 1.0\n",
    "        else:\n",
    "            tensor = np.zeros((1,), dtype=np.float32)\n",
    "            tensor[0] = float(value)\n",
    "        if self.transform:\n",
    "            return self.transform(tensor)\n",
    "        return tensor\n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return self.column\n",
    "    \n",
    "class AgeAtECGWideFileDD(DataDescription):\n",
    "    # DataDescription for a wide file\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        wide_df: pd.DataFrame,\n",
    "        reference_date_column: str,  # e.g. start_fu\n",
    "        reference_age_column: str,  # e.g. start_fu_age\n",
    "    ):\n",
    "        \"\"\"\n",
    "        \"\"\"\n",
    "        self.wide_df = wide_df\n",
    "        self.reference_date_column = reference_date_column\n",
    "        self.reference_age_column = reference_age_column\n",
    "\n",
    "    def get_loading_options(self, sample_id):\n",
    "        row = self.wide_df.loc[[sample_id]].iloc[0]\n",
    "        start_fu_date = row[self.reference_date_column].to_pydatetime()\n",
    "        loading_option = {DATE_OPTION_KEY: start_fu_date}\n",
    "        return [loading_option]\n",
    "\n",
    "    def get_raw_data(self, sample_id, loading_option):\n",
    "        \"\"\"expects time of ECG in the loading option as DATE_OPTION_KEY\"\"\"\n",
    "        ecg_date = loading_option[DATE_OPTION_KEY]\n",
    "        row = self.wide_df.loc[sample_id]\n",
    "        ref_age = row[self.reference_age_column]\n",
    "        ref_date = row[self.reference_date_column]\n",
    "        age_at_ecg = ref_age - (ref_date - ecg_date)\n",
    "        age_in_years = age_at_ecg.total_seconds()/31536000\n",
    "        norm_age = age_in_years - 63.36\n",
    "        norm_age /= 7.55\n",
    "        return np.array(norm_age, dtype=np.float32) \n",
    "    \n",
    "    @property\n",
    "    def name(self):\n",
    "        return \"output_age_from_wide_csv_continuous\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77f93a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build the data description, and make sure it works!\n",
    "sex_from_wide_dd = ColumnFromWideFileDD(\n",
    "    wide_df=wide_df,\n",
    "    column=\"Dem.Gender.no_filter\",\n",
    "    channel_map={'female': 0, 'male': 1}\n",
    ")\n",
    "\n",
    "def bmi_check(x):\n",
    "    if x[0] > 50 or x[0] < 12:\n",
    "        raise ValueError('bmi out of range')\n",
    "    return x\n",
    "\n",
    "wide_dds = []\n",
    "wide_cols = [\n",
    "     'MGH_MRN_0',\n",
    "     'BWH_MRN_0',\n",
    "]\n",
    "for col in wide_cols:\n",
    "    if col == 'start_fu_BMI':\n",
    "        wide_dds.append(ColumnFromWideFileDD(wide_df=wide_df, column=col, transform=bmi_check))\n",
    "    else:\n",
    "        wide_dds.append(ColumnFromWideFileDD(wide_df=wide_df, column=col))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "quarterly-worship",
   "metadata": {},
   "outputs": [],
   "source": [
    "def option_picker(sample_id, dds):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    start_fu = wide_df.loc[sample_id][\"start_fu_date\"]\n",
    "    hospital = wide_df.loc[sample_id][\"hospital\"]\n",
    "    bwh_long_name = \"BRIGHAM & WOMEN'S/FAULKNER HOSP.\"\n",
    "    min_ecg_dt = start_fu - pd.to_timedelta(\"3y\")\n",
    "    max_ecg_dt = start_fu\n",
    "    ecg_dts = ecg_dd_i.get_loading_options(sample_id)\n",
    "    ecg_dts = [\n",
    "        {\n",
    "            DATE_OPTION_KEY: option[DATE_OPTION_KEY],\n",
    "            ecg_dd_i.S3_PATH_OPTION: option[ecg_dd_i.S3_PATH_OPTION],\n",
    "        }\n",
    "        for option in ecg_dts\n",
    "        if min_ecg_dt <= option[DATE_OPTION_KEY] <= max_ecg_dt and \\\n",
    "             ((option['SITE'] == bwh_long_name and hospital == 'BWH') or \\\n",
    "              (hospital in option['SITE'] and hospital == 'MGH'))\n",
    "    ]\n",
    "    if not ecg_dts:\n",
    "        raise ValueError(\"No dates available\")\n",
    "    #dt = np.random.choice(ecg_dts)\n",
    "    dt = ecg_dts[-1]\n",
    "#     summary = ecg_dd_i.get_summary_data(sample_id, dt)\n",
    "#     if summary['num_zeros'] > 1500:\n",
    "#         raise ValueError('too many zeros.')\n",
    "  \n",
    "    return {\n",
    "        dd: dt\n",
    "        for dd in dds\n",
    "    }\n",
    "\n",
    "def option_picker_bwh(sample_id, dds):\n",
    "#     import pdb; pdb.set_trace()\n",
    "    start_fu = wide_df.loc[sample_id][\"start_fu_date\"]\n",
    "    hospital = wide_df.loc[sample_id][\"hospital\"]\n",
    "    bwh_long_name = \"BRIGHAM & WOMEN'S/FAULKNER HOSP.\"\n",
    "    min_ecg_dt = start_fu - pd.to_timedelta(\"3y\")\n",
    "    max_ecg_dt = start_fu\n",
    "    ecg_dts = ecg_dd_i_bwh.get_loading_options(sample_id)\n",
    "    ecg_dts = [\n",
    "        {\n",
    "            DATE_OPTION_KEY: option[DATE_OPTION_KEY],\n",
    "            ecg_dd_i_bwh.S3_PATH_OPTION: option[ecg_dd_i_bwh.S3_PATH_OPTION],\n",
    "        }\n",
    "        for option in ecg_dts\n",
    "        if min_ecg_dt <= option[DATE_OPTION_KEY] <= max_ecg_dt and \\\n",
    "             ((option['SITE'] == bwh_long_name and hospital == 'BWH') or \\\n",
    "              (hospital in option['SITE'] and hospital == 'MGH'))\n",
    "    ]\n",
    "    if not ecg_dts:\n",
    "        raise ValueError(\"No dates available\")\n",
    "    #dt = np.random.choice(ecg_dts)\n",
    "    dt = ecg_dts[-1]\n",
    "#     summary = ecg_dd_i.get_summary_data(sample_id, dt)\n",
    "#     if summary['num_zeros'] > 1500:\n",
    "#         raise ValueError('too many zeros.')\n",
    "  \n",
    "    return {\n",
    "        dd: dt\n",
    "        for dd in dds\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specialized-mixture",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how all of the components are merged together\n",
    "sg = DataDescriptionSampleGetter(\n",
    "    input_data_descriptions=[ecg_dd_i],  # what we want a model to use as input data\n",
    "    output_data_descriptions=wide_dds,  # what we want a model to predict from the input data\n",
    "    option_picker=option_picker,\n",
    ")\n",
    "# This is how all of the components are merged together\n",
    "sg_lead_I = DataDescriptionSampleGetter(\n",
    "    input_data_descriptions=[ecg_dd_i_lead_I],  # what we want a model to use as input data\n",
    "    output_data_descriptions=wide_dds,  # what we want a model to predict from the input data\n",
    "    option_picker=option_picker,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01ccbba1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is how all of the components are merged together\n",
    "sg_bwh = DataDescriptionSampleGetter(\n",
    "    input_data_descriptions=[ecg_dd_i_bwh],  # what we want a model to use as input data\n",
    "    output_data_descriptions=wide_dds,  # what we want a model to predict from the input data\n",
    "    option_picker=option_picker_bwh,\n",
    ")\n",
    "sg_bwh_lead_I = DataDescriptionSampleGetter(\n",
    "    input_data_descriptions=[ecg_dd_i_bwh_lead_I],  # what we want a model to use as input data\n",
    "    output_data_descriptions=wide_dds,  # what we want a model to predict from the input data\n",
    "    option_picker=option_picker_bwh,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5490f",
   "metadata": {},
   "outputs": [],
   "source": [
    "sg_explore_df = pd.read_csv('../../af_survive_explore_all.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b5d624",
   "metadata": {},
   "outputs": [],
   "source": [
    "working_ids = sg_explore_df[sg_explore_df[\"error\"].isna()][\"sample_id\"]\n",
    "dataset = SampleGetterIterableDataset(sample_ids=list(working_ids), sample_getter=sg,\n",
    "                                           get_epoch=SampleGetterIterableDataset.shuffle_get_epoch)\n",
    "\n",
    "dataloader = DataLoader(\n",
    "    dataset, num_workers=14, collate_fn=numpy_collate_fn, batch_size=4,\n",
    ")\n",
    "\n",
    "dataset_lead_I = SampleGetterIterableDataset(sample_ids=list(working_ids), sample_getter=sg_lead_I,\n",
    "                                           get_epoch=SampleGetterIterableDataset.shuffle_get_epoch)\n",
    "dataloader_lead_I = DataLoader(\n",
    "    dataset_lead_I, num_workers=12, collate_fn=numpy_collate_fn, batch_size=4, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa2dbdee",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset_bwh = SampleGetterIterableDataset(sample_ids=list(working_ids), sample_getter=sg_bwh,\n",
    "                                           get_epoch=SampleGetterIterableDataset.shuffle_get_epoch)\n",
    "\n",
    "dataloader_bwh = DataLoader(\n",
    "    dataset_bwh, num_workers=14, collate_fn=numpy_collate_fn, batch_size=32,\n",
    ")\n",
    "dataset_bwh_lead_I = SampleGetterIterableDataset(sample_ids=list(working_ids), sample_getter=sg_bwh_lead_I,\n",
    "                                           get_epoch=SampleGetterIterableDataset.shuffle_get_epoch)\n",
    "dataloader_bwh_lead_I = DataLoader(\n",
    "    dataset_bwh_lead_I, num_workers=12, collate_fn=numpy_collate_fn, batch_size=4, \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e3e56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def space_from_dataloader(dataloader, encoders, tensor_map, tensor_map_median, max_batches=25000):\n",
    "    dataloader_iterator = iter(dataloader)\n",
    "    space_dict = defaultdict(list)\n",
    "    for i in range(max_batches):\n",
    "        try:\n",
    "            data, target = next(dataloader_iterator)\n",
    "            median = ecg_10s_2_median(data[tensor_map.input_name()])\n",
    "            if tensor_map_median.shape[-1] == 1:\n",
    "                encoding = encoders[tensor_map_median].predict(median[:,:,:1]) # Lead I is index 0\n",
    "            else:\n",
    "                encoding = encoders[tensor_map_median].predict(median)\n",
    "            for b in range(encoding.shape[0]):\n",
    "                for i in range(encoding.shape[-1]):\n",
    "                    space_dict[f'latent_{i}'].append(encoding[b,i])\n",
    "            for b in range(encoding.shape[0]):\n",
    "                for k in target:\n",
    "                    if isinstance(target[k][b], np.float32):\n",
    "                        space_dict[f'{k}'].append(target[k][b])\n",
    "                    else:\n",
    "                        space_dict[f'{k}'].append(target[k][b, -1])                        \n",
    "        except StopIteration:\n",
    "            print('loaded all batches')\n",
    "            break\n",
    "    return pd.DataFrame.from_dict(space_dict)\n",
    "def space_from_dataloader(dataloader, merger, tensor_map, tensor_map_median, max_batches=25000):\n",
    "    dataloader_iterator = iter(dataloader)\n",
    "    space_dict = defaultdict(list)\n",
    "    for i in range(max_batches):\n",
    "        try:\n",
    "            data, target = next(dataloader_iterator)\n",
    "            median = ecg_10s_2_median(data[tensor_map.input_name()])\n",
    "            if tensor_map_median.shape[-1] == 1:\n",
    "                encoding = merger.predict(median[:,:,:1]) # Lead I is index 0\n",
    "            else:\n",
    "                encoding = merger.predict(median)\n",
    "            for b in range(encoding.shape[0]):\n",
    "                for i in range(encoding.shape[-1]):\n",
    "                    space_dict[f'latent_{i}'].append(encoding[b,i])\n",
    "            for b in range(encoding.shape[0]):\n",
    "                for k in target:\n",
    "                    if isinstance(target[k][b], np.float32):\n",
    "                        space_dict[f'{k}'].append(target[k][b])\n",
    "                    else:\n",
    "                        space_dict[f'{k}'].append(target[k][b, -1])                        \n",
    "        except StopIteration:\n",
    "            print('loaded all batches')\n",
    "            break\n",
    "    return pd.DataFrame.from_dict(space_dict)\n",
    "#df_ecg_median_ae = space_from_dataloader(dataloader, encoders, ecg_tmap, ecg_tmap_median)\n",
    "df_ecg_median_ae = space_from_dataloader(dataloader, merger, ecg_tmap, ecg_tmap_median)\n",
    "#df_ecg_ae_bwh = space_from_dataloader(dataloader_bwh, encoders, ecg_tmap, ecg_tmap_median)\n",
    "#df_ecg_ae_lead_I = space_from_dataloader(dataloader, encoders_lead_I, ecg_tmap, ecg_tmap_median_lead_I)\n",
    "#df_ecg_ae_bwh_lead_I = space_from_dataloader(dataloader_bwh, encoders_lead_I, ecg_tmap, ecg_tmap_median_lead_I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d207dc64",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_median_ae = df_ecg_median_ae.rename(columns={'MGH_MRN_0': 'sample_id'})\n",
    "model_name = 'mgh_ecg_rest_median_raw_10_autoencoder_256d_v2022_04_13'\n",
    "df_ecg_median_ae.to_csv(f'../../trained_models/{model_name}/merged_mgh_latent_{model_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f8761f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_median_ae.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb99b4b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_ae_bwh = df_ecg_ae_bwh.rename(columns={'BWH_MRN_0': 'sample_id'})\n",
    "model_name = 'mgh_ecg_rest_median_raw_10_autoencoder_256d_v2022_04_13'\n",
    "df_ecg_ae_bwh.to_csv(f'../../trained_models/{model_name}/bwh_latent_{model_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92a82c6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_ae_lead_I = df_ecg_ae_lead_I.rename(columns={'MGH_MRN_0': 'sample_id'})\n",
    "model_name = 'mgh_ecg_rest_median_raw_10_lead_I_autoencoder_256d_v2022_04_09'\n",
    "df_ecg_ae_lead_I.to_csv(f'../../trained_models/{model_name}/mgh_latent_lead_I_{model_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c42fe9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_ae_bwh_lead_I = df_ecg_ae_bwh_lead_I.rename(columns={'BWH_MRN_0': 'sample_id'})\n",
    "model_name = 'mgh_ecg_rest_median_raw_10_lead_I_autoencoder_256d_v2022_04_09'\n",
    "df_ecg_ae_bwh_lead_I.to_csv(f'../../trained_models/{model_name}/bwh_latent_lead_I_{model_name}.tsv', sep='\\t', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8456ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "f'../../trained_models/{model_name}/mgh_latent_{model_name}.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b006c7d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_ae_bwh = df_ecg_ae_bwh.rename(columns={'BWH_MRN_0': 'sample_id'})\n",
    "file_name = f'../../trained_models/{model_name}/bwh_latent_{model_name}.tsv'\n",
    "df_ecg_ae_bwh.to_csv(file_name, sep='\\t', index=False)\n",
    "print(f'Wrote latent space to: {file_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f173b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_ae_bwh.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5450595c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ecg_ae.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "innocent-carolina",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloader_iterator = iter(dataloader)\n",
    "data, target = next(dataloader_iterator)\n",
    "median = ecg_10s_2_median(data[ecg_tmap.input_name()])\n",
    "encoding = encoders[ecg_tmap_median].predict(median)\n",
    "print(f'{encoding.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76ba6eb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoding = decoders[ecg_tmap_median].predict(encoding)\n",
    "print(f'{decoding.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "177eac16",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.linspace(0, 10, 5000), data[ecg_tmap.input_name()][0])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bd0b325",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "plt.plot(np.linspace(0, 10, 600), median[2])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b07e39",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo = ecg_autoencoder(median)\n",
    "plt.plot(np.linspace(0, 10, 600), mo[2])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33376a08",
   "metadata": {},
   "outputs": [],
   "source": [
    "mo2 = decoders[ecg_tmap_median](merger(median))\n",
    "plt.plot(np.linspace(0, 10, 600), mo2[2])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7408cab9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(np.linspace(0, 10, 600), decoding[0])\n",
    "plt.xlabel(\"time (s)\")\n",
    "plt.ylabel(\"amplitude (mV)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11aff99",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_uni = df_uni[df_uni.MGH_MRN_0.notna()]\n",
    "df_uni.MGH_MRN_0 = df_uni.MGH_MRN_0.astype(np.int64)\n",
    "df_uni.set_index(\"MGH_MRN_0\", inplace=True)\n",
    "df_uni.to_csv('../../mgh_drop_fuse_latent_space_uni.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ada0f827",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lvef.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2167528c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../../mgh_drop_fuse_latent_space.csv')\n",
    "df_mv = pd.read_csv('../../mgh_drop_fuse_latent_space_mv.csv')\n",
    "df_uni = pd.read_csv('../../mgh_drop_fuse_latent_space_uni.csv')\n",
    "#df_lvef = pd.read_csv('../../mgh_lvef_latent_space_uni.csv')\n",
    "df_auto = pd.read_csv('../../mgh_auto_df.csv')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9985722",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_scores = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61009e9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "phenotypes=['output_age_from_wide_csv_continuous', 'Dem.Gender.no_filter'] + wide_cols\n",
    "all_scores['zscored'] = latent_space_regression(df, phenotypes, verbose=True)\n",
    "#all_scores['mgb lvef'] = latent_space_regression(df_lvef, phenotypes, num_features=33, verbose=True)\n",
    "all_scores['mgb auto'] = latent_space_regression(df_auto, phenotypes, num_features=256, verbose=True)\n",
    "\n",
    "\n",
    "all_scores['millivolts'] = latent_space_regression(df_mv, phenotypes, verbose=True)\n",
    "all_scores['unimodal_zscored'] = latent_space_regression(df_uni, phenotypes, verbose=True)\n",
    "\n",
    "# all_scores['per_individual_normalized2'] = latent_space_regression(df2, phenotypes, verbose=True)\n",
    "# all_scores['per_individual_normalized_cw712'] = latent_space_regression(df_cw712, phenotypes, verbose=True)\n",
    "# all_scores['millivolts2'] = latent_space_regression(df_mv2, phenotypes, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5be79a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nested_dictionary(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c9df451",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_nested_dictionary(all_scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b53b36",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.start_fu_LVEF.plot.hist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62afd80e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.start_fu_LVEF.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe16291a",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['start_fu_LVEF']].info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b0e0d0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
