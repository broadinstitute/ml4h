{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#mixup.py\n",
    "import math\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.regularizers import l2\n",
    "from keras.utils import to_categorical\n",
    "from keras.datasets import mnist, cifar10\n",
    "from keras.layers import Activation, Input, Dense, Conv2D, LeakyReLU\n",
    "from keras.layers import Dropout, BatchNormalization, Flatten, Reshape, SpatialDropout2D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _mixup_batch(in_batch: np.ndarray, out_batch: np.ndarray, alpha: int = 1.0):\n",
    "    \"\"\"Mixup the batch by sampling from a beta distribution and \n",
    "    computing a weighted average of the first half of the batch with last half.\"\"\"\n",
    "    half = in_batch.shape[0] // 2\n",
    "    mixed_ins = np.zeros((half,) + in_batch.shape[1:])\n",
    "    mixed_outs = np.zeros((half,) + out_batch.shape[1:])\n",
    "    for i in range(half):\n",
    "        weight0 = np.random.beta(alpha, alpha)\n",
    "        weight1 = 1 - weight0\n",
    "        mixed_ins[i] = (in_batch[i, ...] * weight0) + (in_batch[half+i, ...] * weight1)\n",
    "        mixed_outs[i] = (out_batch[i, ...] * weight0) + (out_batch[half+i, ...] * weight1)\n",
    "    return mixed_ins, mixed_outs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "    img_rows, img_cols = 28, 28\n",
    "\n",
    "    # the data, shuffled and split between train and test sets\n",
    "    (x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "    x_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "    x_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1).astype('float32')\n",
    "    x_train /= 128.0\n",
    "    x_test /= 128.0\n",
    "    x_train -= 1.0\n",
    "    x_test -= 1.0\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    print('bounds:', np.min(x_train), np.max(x_train))\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "    print('y_train shape:', y_train.shape)\n",
    "    print('y_train sum:', np.sum(y_train, axis=0))\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_cifar():\n",
    "    (x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
    "    x_train = x_train.astype('float32')\n",
    "    x_test = x_test.astype('float32')\n",
    "    x_train /= 255.0\n",
    "    x_test /= 255.0\n",
    "    #x_train -= 1.0\n",
    "    #x_test -= 1.0\n",
    "    y_train = to_categorical(y_train, 10)\n",
    "    y_test = to_categorical(y_test, 10)\n",
    "    print('bounds:', np.min(x_train), np.max(x_train))\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\t\n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _plot_batch(image_batch):\n",
    "    batch_size = image_batch.shape[0]\n",
    "    sqrt_batch = math.ceil(math.sqrt(batch_size))\n",
    "    _, axes = plt.subplots(sqrt_batch, sqrt_batch, figsize=(18, 14))\n",
    "    for i in range(batch_size):\n",
    "        if image_batch.shape[-1] == 1:\n",
    "            axes[i//sqrt_batch, i%sqrt_batch].imshow(image_batch[i, :, :, 0])\n",
    "        else:\n",
    "            axes[i//sqrt_batch, i%sqrt_batch].imshow(image_batch[i, ...])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminative_model(in_shape, out_classes):\n",
    "    d_input = Input(in_shape)\n",
    "    H = Conv2D(128, (3, 3), strides=(1,1), padding='same')(d_input)\n",
    "    H = BatchNormalization()(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Conv2D(256, (3, 3), strides=(2,2), padding='same')(H)\n",
    "    H = BatchNormalization()(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Conv2D(256, (3, 3), strides=(2,2), padding='same')(H)\n",
    "    H = BatchNormalization()(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Flatten()(H)\n",
    "    H = Dense(128)(H)\n",
    "    H = BatchNormalization()(H)\n",
    "    d_V = Dense(out_classes, activation='softmax')(H)\n",
    "    discriminator = Model(d_input, d_V)\n",
    "    discriminator.compile(loss='categorical_crossentropy', optimizer='adam')\n",
    "    discriminator.summary()\n",
    "    return discriminator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _accuracy_on_batch(discriminator, batch, y):\n",
    "    y_hat = discriminator.predict(batch)\n",
    "    y_hat_idx = np.argmax(y_hat, axis=-1)\n",
    "    y_idx = np.argmax(y, axis=-1)\n",
    "    diff = y_idx-y_hat_idx\n",
    "    n_tot = y.shape[0]\n",
    "    n_rig = (diff==0).sum()\n",
    "    acc = n_rig*100.0/n_tot\n",
    "    print(f'Accuracy: {acc:0.02f} pct ({n_rig} of {n_tot}).')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _train_on_batches(discriminator, x_train, y_train, steps, batch_size, mixup=False, alpha=1.0, steps_per_print=100):\n",
    "    losses = []\n",
    "    if mixup:\n",
    "        batch_size *= 2\n",
    "    for s in range(steps):\n",
    "        indexes = np.random.randint(0, x_train.shape[0], size=batch_size)\n",
    "        image_batch = x_train[indexes, ...] \n",
    "        class_label = y_train[indexes, ...]\n",
    "        if mixup:\n",
    "            image_batch, class_label = _mixup_batch(image_batch, class_label, alpha)\n",
    "        losses.append(discriminator.train_on_batch(image_batch, class_label))\n",
    "        if s%steps_per_print == 0:\n",
    "            _accuracy_on_batch(discriminator, image_batch, class_label)\n",
    "    return losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(mnist_x, mnist_y), (mnist_test_x, mnist_test_y) = load_mnist()\n",
    "mnist_discriminator = build_discriminative_model((28, 28, 1), 10)\n",
    "losses = {}\n",
    "losses['erm'] = _train_on_batches(mnist_discriminator, mnist_x, mnist_y, 2000, 256, mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_discriminator = build_discriminative_model((28, 28, 1), 10)\n",
    "losses['mixup_0.1'] = _train_on_batches(mnist_discriminator, mnist_x, mnist_y, 2000, 256, mixup=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_discriminator = build_discriminative_model((28, 28, 1), 10)\n",
    "losses['mixup_0.5'] = _train_on_batches(mnist_discriminator, mnist_x, mnist_y, 1000, 64, mixup=True, alpha=0.5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in losses:\n",
    "    plt.plot(losses[k], label=k)\n",
    "plt.legend(list(losses.keys()), loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(cifar_x, cifar_y), _ = load_cifar()\n",
    "cifar_discriminator = build_discriminative_model((32, 32, 3), 10)\n",
    "cifar_losses = {}\n",
    "cifar_losses['erm'] = _train_on_batches(cifar_discriminator, cifar_x, cifar_y, 5000, 256, mixup=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_discriminator = build_discriminative_model((32, 32, 3), 10)\n",
    "cifar_losses['mixup_0.1'] = _train_on_batches(cifar_discriminator, cifar_x, cifar_y, 5000, 256, mixup=True, alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_discriminator = build_discriminative_model((32, 32, 3), 10)\n",
    "cifar_losses['mixup_0.3'] = _train_on_batches(cifar_discriminator, cifar_x, cifar_y, 2000, 128, mixup=True, alpha=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in cifar_losses:\n",
    "    plt.plot(cifar_losses[k], label=k)\n",
    "plt.legend(list(cifar_losses.keys()), loc='upper right')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cifar_discriminator = build_discriminative_model((32, 32, 3), 10)\n",
    "cifar_losses['mixup_1.0'] = _train_on_batches(cifar_discriminator, cifar_x, cifar_y, 2000, 128, mixup=True, alpha=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnist_discriminator = build_discriminative_model((28, 28, 1), 10)\n",
    "_train_on_batches(mnist_discriminator, mnist_x, mnist_y, 1000, 9, mixup=False, steps_per_print=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_mnist_model = build_discriminative_model((28, 28, 1), 10)\n",
    "_train_on_batches(mixup_mnist_model, mnist_x, mnist_y, 10000, 9, mixup=True, alpha=500.0, steps_per_print=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mixup_mnist_model = build_discriminative_model((28, 28, 1), 10)\n",
    "_train_on_batches(mixup_mnist_model, mnist_x, mnist_y, 10000, 32, mixup=True, steps_per_print=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
