{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#gan.py\n",
    "import math\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Model\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.datasets import mnist, cifar10\n",
    "from tensorflow.keras.regularizers import l1, l2\n",
    "from tensorflow.keras.utils import plot_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, BatchNormalization, Activation, Flatten, Reshape, SpatialDropout2D\n",
    "from tensorflow.keras.layers import Activation, Input, Dense, Conv2D, Conv2DTranspose, BatchNormalization, LeakyReLU, GaussianNoise\n",
    "from tensorflow.keras.losses import mse, binary_crossentropy, kullback_leibler_divergence\n",
    "from tensorflow.keras.optimizers import Adam, RMSprop\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_mnist():\n",
    "\timg_rows, img_cols = 28, 28\n",
    "\n",
    "\t# the data, shuffled and split between train and test sets\n",
    "\t(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
    "\tx_train = x_train.reshape(x_train.shape[0], img_rows, img_cols, 1)\n",
    "\tx_test = x_test.reshape(x_test.shape[0], img_rows, img_cols, 1)\n",
    "\tx_train = x_train.astype('float32')\n",
    "\tx_test = x_test.astype('float32')\n",
    "\tx_train /= 128.0\n",
    "\tx_test /= 128.0\n",
    "\tx_train -= 1.0\n",
    "\tx_test -= 1.0\n",
    "\tprint('bounds:', np.min(x_train), np.max(x_train))\n",
    "\tprint('x_train shape:', x_train.shape)\n",
    "\tprint(x_train.shape[0], 'train samples')\n",
    "\tprint(x_test.shape[0], 'test samples')\n",
    "\treturn (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "seeds = 100\n",
    "first_channels = 36\n",
    "dropout_rate = 0.0\n",
    "opt = Adam(lr=2e-4, beta_1=0.5, beta_2=0.999, epsilon=1e-08)\n",
    "dopt = Adam(lr=2e-4, beta_1=0.5, beta_2=0.999, epsilon=1e-08)\n",
    "def build_generative_model():\n",
    "    g_input = Input(shape=[seeds])\n",
    "    H = Dense(7*7*first_channels, activation='relu')(g_input)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Reshape([7, 7, first_channels])(H)\n",
    "    H = Conv2DTranspose(256, (3, 3), strides=2, padding='same')(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Conv2DTranspose(128, (3, 3), strides=2, padding='same')(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Conv2D(64, (3, 3), strides=1, padding='same')(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    pre_logit = Conv2D(1, (1, 1), padding='same', kernel_regularizer=l2(0.0001), activity_regularizer=l1(0.00002))(H)\n",
    "    pixel = Activation('tanh')(pre_logit)\n",
    "\n",
    "    generator = Model(g_input, pixel)\n",
    "    generator.compile(loss='mse', optimizer=opt)\n",
    "    generator.summary()\n",
    "    return generator\n",
    "\n",
    "def build_dense_generative_model():\n",
    "    g_input = Input(shape=[seeds])\n",
    "    H = Dense(28)(g_input)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Dense(784)(g_input)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = Activation('relu')(H)\n",
    "    H = Dropout(dropout_rate)(H)\n",
    "    H = Reshape([28, 28, 1])(H)\n",
    "    pre_logit = Conv2D(1, (1, 1), padding='same')(H) #, kernel_regularizer=l2(0.0001), activity_regularizer=l1(0.00002))(H)\n",
    "    pixel = Activation('tanh')(pre_logit)\n",
    "\n",
    "    generator = Model(g_input, pixel)\n",
    "    generator.compile(loss='mse', optimizer=opt)\n",
    "    generator.summary()\n",
    "    return generator\n",
    "generator = build_dense_generative_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_discriminative_model(in_shape):\n",
    "    d_input = Input(in_shape)\n",
    "    H = Conv2D(128, (3, 3), strides=(2,2), padding='same', kernel_regularizer=l2(0.001))(d_input)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Conv2D(64, (3, 3), strides=(2,2), padding='same', kernel_regularizer=l2(0.001))(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Flatten()(H)\n",
    "    H = Dense(32, kernel_regularizer=l2(0.001))(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    d_V = Dense(2, activation='softmax', kernel_regularizer=l2(0.001))(H)\n",
    "    discriminator = Model(d_input, d_V)\n",
    "    #discriminator.compile(loss='categorical_crossentropy', optimizer=dopt)\n",
    "    discriminator.summary()\n",
    "    return discriminator\n",
    "\n",
    "def build_dense_discriminative_model(in_shape):\n",
    "    d_input = Input(in_shape)\n",
    "    H = Flatten()(d_input)\n",
    "    H = Dense(64)(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    H = Dense(32)(H)\n",
    "    H = BatchNormalization(momentum=0.9)(H)\n",
    "    H = LeakyReLU(0.2)(H)\n",
    "    d_V = Dense(2, activation='softmax', kernel_regularizer=l2(0.001))(H)\n",
    "    discriminator = Model(d_input, d_V)\n",
    "    #discriminator.compile(loss='categorical_crossentropy', optimizer=dopt)\n",
    "    discriminator.summary()\n",
    "    return discriminator\n",
    "discriminator = build_dense_discriminative_model((28, 28, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_stacked_gan(generator, discriminator):\n",
    "    discriminator.trainable = False\n",
    "    gan_input = Input(shape=[seeds])\n",
    "    H = generator(gan_input)\n",
    "    gan_V = discriminator(H)\n",
    "    GAN = Model(gan_input, gan_V)\n",
    "    GAN.compile(loss='categorical_crossentropy', optimizer=opt)\n",
    "    GAN.summary()\n",
    "    return GAN\n",
    "gan = build_stacked_gan(generator, discriminator)\n",
    "discriminator.trainable = True\n",
    "discriminator.compile(loss='categorical_crossentropy', optimizer=dopt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrain_discriminator(x_train, generator, discriminator, iterations):\n",
    "    trainidx = random.sample(range(0, x_train.shape[0]), iterations)\n",
    "    xt = x_train[trainidx,:,:,:]\n",
    "    noise_gen = np.random.uniform(0, 1, size=[xt.shape[0], seeds])\n",
    "    generated_images = generator.predict(noise_gen)\n",
    "    x = np.concatenate((xt, generated_images))\n",
    "    n = xt.shape[0]\n",
    "    y = np.zeros([2*n, 2])\n",
    "    y[:n, 0] = 1\n",
    "    y[n:, 1] = 1\n",
    "    print('np sum 1s:', np.sum(y[:,0]), 'x shape:', x.shape)\n",
    "    discriminator.fit(x,y, epochs=1, batch_size=32, validation_split=0.1, shuffle=True)\n",
    "    y_hat = discriminator.predict(x)\n",
    "\n",
    "    # Measure accuracy of pre-trained discriminator network\n",
    "    y_hat_idx = np.argmax(y_hat,axis=1)\n",
    "    y_idx = np.argmax(y,axis=1)\n",
    "    diff = y_idx-y_hat_idx\n",
    "    n_tot = y.shape[0]\n",
    "    n_rig = (diff==0).sum()\n",
    "    acc = n_rig*100.0/n_tot\n",
    "    print(\"Accuracy: %0.02f pct (%d of %d) right\"%(acc, n_rig, n_tot))\n",
    "\n",
    "def pretrain_generator(x_train, gan, generator, discriminator, iterations):\n",
    "    batch_size = 64  \n",
    "    for i in range(iterations):\n",
    "        noise_tr = np.random.uniform(0, 1, size=[batch_size, seeds])\n",
    "        y2 = np.zeros([batch_size, 2])\n",
    "        # Tell the model that random is correct\n",
    "        y2[:, 0] = 1\n",
    "        g_loss = gan.train_on_batch(noise_tr, y2)\n",
    "        if i%50 == 0:\n",
    "            print(f\"Pretrain gen it: {i}, Generator loss {g_loss}\")\n",
    "    \n",
    "    trainidx = random.sample(range(0, x_train.shape[0]), iterations)\n",
    "    xt = x_train[trainidx,:,:,:]\n",
    "    noise_gen = np.random.uniform(0, 1, size=[xt.shape[0], seeds])\n",
    "    generated_images = generator.predict(noise_gen)\n",
    "    x = np.concatenate((xt, generated_images))\n",
    "    n = xt.shape[0]\n",
    "    y = np.zeros([2*n, 2])\n",
    "    y[:n, 0] = 1\n",
    "    y[n:, 1] = 1\n",
    "    print('np sum 1s:', np.sum(y[:,0]), 'x shape:', x.shape)\n",
    "    y_hat = discriminator.predict(x)\n",
    "\n",
    "    # Measure accuracy of pre-trained discriminator network\n",
    "    y_hat_idx = np.argmax(y_hat,axis=1)\n",
    "    y_idx = np.argmax(y,axis=1)\n",
    "    diff = y_idx-y_hat_idx\n",
    "    n_tot = y.shape[0]\n",
    "    n_rig = (diff==0).sum()\n",
    "    acc = n_rig*100.0/n_tot\n",
    "    print(\"Accuracy: %0.02f pct (%d of %d) right\"%(acc, n_rig, n_tot))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_trainable(net, val):\n",
    "\tnet.trainable = val\n",
    "\tfor l in net.layers:\n",
    "\t\tl.trainable = val\n",
    "\n",
    "def plot_gen_color(generator, n_ex=16, dim=(4,4), figsize=(24,24), random_seeds=None, save_path=None):\n",
    "    if random_seeds is None:\n",
    "        random_seeds = np.random.uniform(0,1,size=[n_ex, seeds])\n",
    "    generated_images = generator.predict(random_seeds)\n",
    "\n",
    "    fig = plt.figure(figsize=figsize)\n",
    "    for i in range(generated_images.shape[0]):\n",
    "        plt.subplot(dim[0],dim[1],i+1)\n",
    "        img = generated_images[i, :, :, 0]\n",
    "        img += 1.0\n",
    "        img /= 2.0\n",
    "        plt.imshow(img)\n",
    "        plt.axis('off')\n",
    "    plt.tight_layout()\n",
    "    if save_path:\n",
    "        if not os.path.exists(os.path.dirname(save_path)):\n",
    "            os.makedirs(os.path.dirname(save_path))\n",
    "        plt.savefig(save_path)\n",
    "        plt.close(fig)\n",
    "    else:\n",
    "        plt.show()\n",
    "        \n",
    "        \n",
    "def train_for_n(epochs, data, generator, discriminator, gan):\n",
    "    # set up loss storage vector\n",
    "    losses = {\"d\":[], \"g\":[]}\n",
    "    (x_train, y_train), (x_test, y_test) = data\n",
    "    print('bounds:', np.min(x_train), np.max(x_train))\n",
    "    print('x_train shape:', x_train.shape)\n",
    "    print(x_train.shape[0], 'train samples')\n",
    "    print(x_test.shape[0], 'test samples')\n",
    "\n",
    "    samples_seeds = np.random.uniform(0, 1, size=[16, seeds])\n",
    "    batch_size = 256  \n",
    "    \n",
    "    for e in range(epochs):\n",
    "        # Make generative images\n",
    "        noise_gen = np.random.uniform(0, 1, size=[batch_size, seeds])\n",
    "        generated_images = generator.predict(noise_gen)\n",
    "        image_batch = x_train[np.random.randint(0, x_train.shape[0], size=batch_size),:,:,:]  \n",
    "        # Train discriminator on generated images\n",
    "        X = np.concatenate((image_batch, generated_images))\n",
    "        y = np.zeros([2*batch_size, 2])\n",
    "        y[:] = 0.0\n",
    "        y[:batch_size, 0] = 0.9\n",
    "        y[batch_size:, 1] = 0.9\n",
    "        discriminator.trainable = True\n",
    "        d_loss = discriminator.train_on_batch(X,y)\n",
    "        losses[\"d\"].append(d_loss)\n",
    "        discriminator.trainable = False\n",
    "\n",
    "        # train Generator-Discriminator stack on input noise to non-generated output class\n",
    "        noise_tr = np.random.uniform(0, 1, size=[batch_size, seeds])\n",
    "        y2 = np.zeros([batch_size, 2])\n",
    "        # Tell the model that random is correct\n",
    "        y2[:, 0] = 1\n",
    "        g_loss = gan.train_on_batch(noise_tr, y2)\n",
    "        losses[\"g\"].append(g_loss)\n",
    "        #print(f\"Epoch {e} gen it: {i}, Generator loss {g_loss}\")\n",
    "        if e% 100 == 0:\n",
    "            print(f\"Epoch {e}, Generator loss {g_loss}, discriminator loss {d_loss}\")\n",
    "            plot_gen_color(generator, random_seeds=samples_seeds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_mnist()\n",
    "#pretrain_discriminator(data[0][0], generator, discriminator, 1200)\n",
    "#pretrain_generator(data[0][0], gan, generator, discriminator, 100)\n",
    "train_for_n(150000, data, generator, discriminator, gan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
