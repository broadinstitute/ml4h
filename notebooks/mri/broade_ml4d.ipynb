{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Welcome to ML4D BroadE!\n",
    "We explore machine learning on Bio medical data using Cloud computing, Python, Tensorflow, and the ml4h codebase.\n",
    "\n",
    "We will start with linear regression.  Our model is a vector, one weight for each input feature, and a single bias weight.\n",
    "\n",
    "\\begin{equation}\n",
    "y = xw + b\n",
    "\\end{equation}\n",
    "\n",
    "For notational convenience absorb the bias term into the weight vector by adding a 1 to the input data matrix X\n",
    "\n",
    "\\begin{equation}\n",
    "y = [1, x][b, w]^T\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import h5py\n",
    "import numpy as np\n",
    "from typing import List, Dict, Callable\n",
    "from collections import defaultdict\n",
    "\n",
    "# Import:\n",
    "import os\n",
    "import sys\n",
    "import gzip\n",
    "import pickle\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "\n",
    "from keras import metrics\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense\n",
    "\n",
    "from ml4h.arguments import _get_tmap\n",
    "from ml4h.TensorMap import TensorMap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_regression():\n",
    "    samples = 40\n",
    "    real_weight = 2.0\n",
    "    real_bias = 0.5\n",
    "    x = np.linspace(-1, 1, samples)\n",
    "    y = real_weight*x + real_bias + (np.random.randn(*x.shape) * 0.1)\n",
    "\n",
    "    linear_model = Sequential()\n",
    "    linear_model.add(Dense(1, input_dim=1))\n",
    "    linear_model.compile(loss='mse', optimizer='sgd')\n",
    "    linear_model.summary()\n",
    "    linear_model.fit(x, y, batch_size=1, epochs=10)\n",
    "\n",
    "    learned_slope = linear_model.get_weights()[0][0][0]\n",
    "    learned_bias = linear_model.get_weights()[1][0]\n",
    "    print('Learned slope:',  learned_slope, 'real slope:', real_weight, 'learned bias:', learned_bias, 'real bias:', real_bias)\n",
    "\n",
    "    plt.plot(x, y)\n",
    "    plt.plot([-1,1], [-learned_slope+learned_bias, learned_slope+learned_bias], 'r')\n",
    "    plt.show()\n",
    "    print('Linear Regression complete!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now Logistic Regression:\n",
    "We take the real-valued predictions from linear regression and squish them with a sigmoid.\n",
    "\n",
    "\\begin{equation}\n",
    "\\textbf{y} = \\sigma(X\\textbf{w} + b)\n",
    "\\end{equation}\n",
    "\n",
    "where \n",
    "\\begin{equation}\n",
    "\\sigma(x) = \\frac{e^x}{1+e^x}\n",
    "\\end{equation}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def sigmoid(x):\n",
    "    a = []\n",
    "    for item in x:\n",
    "        a.append(math.exp(item)/(1+math.exp(item)))\n",
    "    return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(-10., 10., 0.2)\n",
    "sig = sigmoid(x)\n",
    "plt.plot(x,sig)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logistic_regression():\n",
    "\ttrain, test, valid = load_data('mnist.pkl.gz')\n",
    "\n",
    "\tepochs = 1000\n",
    "\tnum_labels = 10\n",
    "\ttrain_y = make_one_hot(train[1], num_labels)\n",
    "\tvalid_y = make_one_hot(valid[1], num_labels)\n",
    "\ttest_y = make_one_hot(test[1], num_labels)\n",
    "\n",
    "\tlogistic_model = Sequential()\n",
    "\tlogistic_model.add(Dense(10, activation='softmax', input_dim=784, name='mnist_templates'))\n",
    "\tlogistic_model.compile(loss='categorical_crossentropy', optimizer='sgd', metrics=['accuracy'])\n",
    "\tlogistic_model.summary()\n",
    "\ttemplates = logistic_model.layers[0].get_weights()[0]\n",
    "\tplot_templates(templates, 0)\n",
    "\tprint('weights shape:', templates.shape)\n",
    "\n",
    "\tfor e in range(epochs):\n",
    "\t\ttrainidx = random.sample(range(0, train[0].shape[0]), 8192)\n",
    "\t\tx_batch = train[0][trainidx,:]\n",
    "\t\ty_batch = train_y[trainidx]\n",
    "\t\tlogistic_model.train_on_batch(x_batch, y_batch)\n",
    "\t\tif e % 100 == 0:\n",
    "\t\t\tplot_templates(logistic_model.layers[0].get_weights()[0], e)\n",
    "\t\t\tprint('Logistic Model test set loss and accuracy:', logistic_model.evaluate(test[0], test_y), 'at epoch', e)\n",
    "\n",
    "\n",
    "def plot_templates(templates, epoch):\n",
    "\tn = 10\n",
    "\ttemplates = templates.reshape((28,28,n))\n",
    "\tplt.figure(figsize=(16, 8))\n",
    "\tfor i in range(n):\n",
    "\t\tax = plt.subplot(2, 5, i+1)\t\t\n",
    "\t\tplt.imshow(templates[:, :, i])\n",
    "\t\tplt.gray()\n",
    "\t\tax.get_xaxis().set_visible(False)\n",
    "\t\tax.get_yaxis().set_visible(False)\n",
    "\n",
    "\tplot_name = \"./regression_example/mnist_templates_\"+str(epoch)+\".png\"\n",
    "\tif not os.path.exists(os.path.dirname(plot_name)):\n",
    "\t\tos.makedirs(os.path.dirname(plot_name))\t\t\n",
    "\tplt.savefig(plot_name)\n",
    "\n",
    "\n",
    "def make_one_hot(y, num_labels):\n",
    "\tohy = np.zeros((len(y), num_labels))\n",
    "\tfor i in range(0, len(y)):\n",
    "\t\tohy[i][y[i]] = 1.0\n",
    "\treturn ohy\n",
    "\n",
    "\n",
    "def load_data(dataset):\n",
    "\t''' Loads the dataset\n",
    "\t:type dataset: string\n",
    "\t:param dataset: the path to the dataset (here MNIST)\n",
    "\t'''\n",
    "\n",
    "\t#############\n",
    "\t# LOAD DATA #\n",
    "\t#############\n",
    "\n",
    "\t# Download the MNIST dataset if it is not present\n",
    "\tdata_dir, data_file = os.path.split(dataset)\n",
    "\tif data_dir == \"\" and not os.path.isfile(dataset):\n",
    "\t\t# Check if dataset is in the data directory.\n",
    "\t\tnew_path = os.path.join(\"data\", dataset)\n",
    "\t\tif os.path.isfile(new_path) or data_file == 'mnist.pkl.gz':\n",
    "\t\t\tdataset = new_path\n",
    "\n",
    "\tif (not os.path.isfile(dataset)) and data_file == 'mnist.pkl.gz':\n",
    "\t\tfrom urllib.request import urlretrieve\n",
    "\t\torigin = (\n",
    "\t\t\t'http://www.iro.umontreal.ca/~lisa/deep/data/mnist/mnist.pkl.gz'\n",
    "\t\t)\n",
    "\t\tprint('Downloading data from %s' % origin)\n",
    "\t\tif not os.path.exists(os.path.dirname(dataset)):\n",
    "\t\t\tos.makedirs(os.path.dirname(dataset))\t\n",
    "\t\turlretrieve(origin, dataset)\n",
    "\n",
    "\tprint('loading data...')\n",
    "\n",
    "\t# Load the dataset\n",
    "\tf = gzip.open(dataset, 'rb')\n",
    "\tif sys.version_info[0] == 3:\n",
    "\t\tu = pickle._Unpickler(f)\n",
    "\t\tu.encoding = 'latin1'\n",
    "\t\ttrain_set, valid_set, test_set = u.load()\n",
    "\telse:\n",
    "\t\ttrain_set, valid_set, test_set = pickle.load(f)\n",
    "\n",
    "\tf.close()\n",
    "\t#train_set, valid_set, test_set format: tuple(input, target)\n",
    "\t#input is an numpy.ndarray of 2 dimensions (a matrix)\n",
    "\t#which row's correspond to an example. target is a\n",
    "\t#numpy.ndarray of 1 dimensions (vector)) that have the same length as\n",
    "\t#the number of rows in the input. It should give the target\n",
    "\t#target to the example with the same index in the input.\n",
    "\n",
    "\treturn train_set, valid_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "logistic_regression()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TensorMaps\n",
    "The critical data structure in the ml4h codebase is the TensorMap.\n",
    "This abstraction provides a way to translate ***any*** kind of input data, into structured numeric tensors with clear semantics for interpretation and modeling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Model Factory\n",
    "The function ***make_multimodal_multitask_model()*** takes lists of TensorMaps and connects them with intelligent goo."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Tensorization\n",
    "Is the process of gathering any number of input files and consolidating them into compressed HD5 files.  We tend to make one HD5 file per sample in the study.  The files contain the raw data and labels we will use to train models.  It tends to be efficient to separate tensor construction from model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tmaps_by_sample_id(tensor_folder: str, sample_id: str, tmaps: List[TensorMap]):\n",
    "    path = os.path.join(tensor_folder, sample_id + '.hd5')\n",
    "    result_dict = defaultdict(lambda: None)\n",
    "    if os.path.isfile(path):\n",
    "            with h5py.File(path, 'r') as hd5:\n",
    "                for tmap in tmaps:\n",
    "                    try:\n",
    "                        result_dict[tmap] = tmap.tensor_from_file(tmap, hd5)\n",
    "                    except (IndexError, KeyError, ValueError, OSError, RuntimeError):\n",
    "                        continue\n",
    "    return result_dict\n",
    "\n",
    "\n",
    "def tmaps_with_properties(tensor_folder: str, tmap_properties: Dict[TensorMap, Callable[[np.ndarray], bool]], search_size=100):\n",
    "    all_ids = [file.strip('.hd5') for file in sorted(os.listdir(tensor_folder))[:search_size]]\n",
    "    results = map(lambda sample_id: tmaps_by_sample_id(tensor_folder, sample_id, tmap_properties.keys()), all_ids)\n",
    "    return {\n",
    "        sample_id: result\n",
    "        for sample_id, result in zip(all_ids, results)\n",
    "        if all(\n",
    "            result[tmap] is not None and tmap_properties[tmap](result[tmap])\n",
    "            for tmap in tmap_properties.keys()\n",
    "        )\n",
    "    }\n",
    "\n",
    "\n",
    "def tmaps_with_properties_from_keys(tensor_folder: str, tmap_properties: Dict[str, Callable[[np.ndarray], bool]], search_size=100):\n",
    "    return tmaps_with_properties(\n",
    "        tensor_folder,\n",
    "        {_get_tmap(key): prop for key, prop in tmap_properties.items()},\n",
    "        search_size,\n",
    "    )\n",
    "\n",
    "\n",
    "def plot_array_3d(a):\n",
    "    slice_axis = -1\n",
    "    sides = int(np.ceil(np.sqrt(a.shape[slice_axis])))\n",
    "    _, axes = plt.subplots(sides, sides, figsize=(16, 16))\n",
    "    print(a.shape)\n",
    "    vmin = np.min(a)\n",
    "    vmax = np.max(a)\n",
    "    for i in range(a.shape[slice_axis]):\n",
    "        axes[i//sides, i%sides].imshow(a[..., i], cmap='gray', vmin=vmin, vmax=vmax)\n",
    "        axes[i//sides, i%sides].set_yticklabels([])\n",
    "        axes[i//sides, i%sides].set_xticklabels([])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmaps = [_get_tmap('t1_30_slices'), _get_tmap('t1_dicom_30_slices'),_get_tmap('t2_flair_30_slices'), _get_tmap('t2_dicom_30_slices')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t = tmaps_by_sample_id('/mnt/disks/brain-tensors-all-40k/2020-01-14/', '3035859', tmaps)\n",
    "for k in t:\n",
    "    print(k.name, 'has', t[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for k in t:\n",
    "    plot_array_3d(t[k])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The Erowid Data Scrape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "http://human.brain-map.org/mri_viewers/data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
