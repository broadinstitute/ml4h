{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Keras imports\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from tensorflow.keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Permute, Reshape, multiply, concatenate, RepeatVector\n",
    "\n",
    "# ML4CVD Imports\n",
    "from ml4cvd.arguments import parse_args\n",
    "from ml4cvd.models import train_model_from_generators\n",
    "\n",
    "# IPython imports\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sam/europarl-v7.es-en.en', 'r', encoding='utf-8') as english_file:\n",
    "    with open('/home/sam/europarl-v7.es-en.es', 'r', encoding='utf-8') as spanish_file:\n",
    "        english = english_file.read()\n",
    "        spanish = spanish_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = '\\t'\n",
    "end_char = '\\n'\n",
    "english_lines = english.split('\\n')\n",
    "spanish_lines = [start_char + l + end_char for l in spanish.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_chars = sorted(list(set(english)))\n",
    "print('total english chars:', len(english_chars))\n",
    "spanish_chars = sorted(list(set(spanish + start_char + end_char)))\n",
    "print('total spanish chars:', len(spanish_chars))\n",
    "chars = sorted(list(set(english_chars + spanish_chars)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_english = 100 # max(map(len, english_lines))//2\n",
    "max_spanish = 100 # max(map(len, spanish_lines))//2\n",
    "max_pairs = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_index = {char: i for i, char in enumerate(english_chars)}\n",
    "spanish_token_index = {char: i for i, char in enumerate(spanish_chars)}\n",
    "english_index_tokens = {i: char for i, char in enumerate(english_chars)}\n",
    "spanish_index_tokens = {i: char for i, char in enumerate(spanish_chars)}\n",
    "encoder_input_data = np.zeros((max_pairs, max_english, len(english_chars)), dtype='float32')\n",
    "decoder_input_data = np.zeros((max_pairs, max_spanish, len(spanish_chars)), dtype='float32')\n",
    "decoder_target_data = np.zeros((max_pairs, max_spanish, len(spanish_chars)), dtype='float32')\n",
    "\n",
    "for i, (english_text, spanish_text) in enumerate(zip(english_lines, spanish_lines)):\n",
    "    for t, char in enumerate(english_text):\n",
    "        if t == max_english:\n",
    "            break\n",
    "        encoder_input_data[i, t, english_token_index[char]] = 1.\n",
    "    for t, char in enumerate(spanish_text):\n",
    "        if t == max_spanish:\n",
    "            break\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, spanish_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, spanish_token_index[char]] = 1.\n",
    "    if i == max_pairs-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64\n",
    "epochs = 6\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, len(english_chars)))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, len(spanish_chars)))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(spanish_chars), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# ~~~~~ Sample with Temperature ~~~~~\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def sample(preds, temperature=1.0):\n",
    "\t# helper function to sample an index from a probability array\n",
    "\tpreds = np.asarray(preds).astype('float64')\n",
    "\tpreds = np.log(preds) / temperature\n",
    "\texp_preds = np.exp(preds)\n",
    "\tpreds = exp_preds / np.sum(exp_preds)\n",
    "\tprobas = np.random.multinomial(1, preds, 1)\n",
    "\treturn np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(spanish_chars)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, spanish_token_index[start_char]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = spanish_index_tokens[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        #print('sampled char is:', sampled_token_index, sampled_char)\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == end_char or len(decoded_sentence) > max_spanish):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(spanish_chars)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', english_lines[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Actual sentence:', spanish_lines[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs, time_steps):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, time_steps))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64\n",
    "epochs = 96\n",
    "from keras.layers import RepeatVector, concatenate\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(max_english, len(english_chars)))\n",
    "encoder_out = LSTM(latent_dim)(encoder_inputs)\n",
    "encoder_repeat = RepeatVector(max_spanish)(encoder_out)\n",
    "attended_encoding = attention_3d_block(encoder_repeat, max_spanish)\n",
    "\n",
    "burn_in = Input(shape=(max_spanish, len(spanish_chars)))\n",
    "lstm_in = layers.concatenate([burn_in, attended_encoding], name='concat_spanish_and_attended')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(lstm_in)\n",
    "decoder_outputs = Dense(len(spanish_chars), activation='softmax')(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, burn_in], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s_attention_96.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define sampling models\n",
    "attn_encoder_model = Model(encoder_inputs, attended_encoding)\n",
    "attn_encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_attended_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    attended_np = attn_encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    burn_in_np = np.zeros((1, max_spanish, len(spanish_chars)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    burn_in_np[0, 0, spanish_token_index[start_char]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    char_idx = 0\n",
    "    while not stop_condition:\n",
    "        char_idx += 1\n",
    "        output_tokens = model.predict([input_seq, burn_in_np])\n",
    "        # Sample a token\n",
    "        sampled_token_index =  sample(output_tokens[0, char_idx-1, :], 1.1)\n",
    "        sampled_char = spanish_index_tokens[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        burn_in_np[0, char_idx, sampled_token_index] = 1.\n",
    "        \n",
    "        \n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == end_char or len(decoded_sentence) > max_spanish) or char_idx == max_spanish-1:\n",
    "            stop_condition = True\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_attended_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', english_lines[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Actual sentence:', spanish_lines[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
