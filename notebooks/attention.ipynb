{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "/usr/local/lib/python3.6/dist-packages/apache_beam/__init__.py:84: UserWarning: Running the Apache Beam SDK on Python 3 is not yet fully supported. You may encounter buggy behavior or missing features.\n",
      "  'Running the Apache Beam SDK on Python 3 is not yet fully supported. '\n",
      "WARNING:root:no GCS storage client\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import math\n",
    "import argparse\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "# Keras imports\n",
    "from keras import layers\n",
    "from keras import backend as K\n",
    "from keras.models import Model\n",
    "from keras.layers import Input, LSTM, Dense, Permute, Reshape, multiply, concatenate, RepeatVector\n",
    "\n",
    "# ML4CVD Imports\n",
    "from ml4cvd.arguments import parse_args\n",
    "from ml4cvd.models import make_multimodal_to_multilabel_model, train_model_from_generators\n",
    "\n",
    "# IPython imports\n",
    "from IPython.display import Image\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('/home/sam/europarl-v7.es-en.en', 'r', encoding='utf-8') as english_file:\n",
    "    with open('/home/sam/europarl-v7.es-en.es', 'r', encoding='utf-8') as spanish_file:\n",
    "        english = english_file.read()\n",
    "        spanish = spanish_file.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "start_char = '\\t'\n",
    "end_char = '\\n'\n",
    "english_lines = english.split('\\n')\n",
    "spanish_lines = [start_char + l + end_char for l in spanish.split('\\n')]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total english chars: 314\n",
      "total spanish chars: 334\n",
      "total chars: 339\n"
     ]
    }
   ],
   "source": [
    "english_chars = sorted(list(set(english)))\n",
    "print('total english chars:', len(english_chars))\n",
    "spanish_chars = sorted(list(set(spanish + start_char + end_char)))\n",
    "print('total spanish chars:', len(spanish_chars))\n",
    "chars = sorted(list(set(english_chars + spanish_chars)))\n",
    "print('total chars:', len(chars))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_english = 100 # max(map(len, english_lines))//2\n",
    "max_spanish = 100 # max(map(len, spanish_lines))//2\n",
    "max_pairs = 15000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "english_token_index = {char: i for i, char in enumerate(english_chars)}\n",
    "spanish_token_index = {char: i for i, char in enumerate(spanish_chars)}\n",
    "english_index_tokens = {i: char for i, char in enumerate(english_chars)}\n",
    "spanish_index_tokens = {i: char for i, char in enumerate(spanish_chars)}\n",
    "encoder_input_data = np.zeros((max_pairs, max_english, len(english_chars)), dtype='float32')\n",
    "decoder_input_data = np.zeros((max_pairs, max_spanish, len(spanish_chars)), dtype='float32')\n",
    "decoder_target_data = np.zeros((max_pairs, max_spanish, len(spanish_chars)), dtype='float32')\n",
    "\n",
    "for i, (english_text, spanish_text) in enumerate(zip(english_lines, spanish_lines)):\n",
    "    for t, char in enumerate(english_text):\n",
    "        if t == max_english:\n",
    "            break\n",
    "        encoder_input_data[i, t, english_token_index[char]] = 1.\n",
    "    for t, char in enumerate(spanish_text):\n",
    "        if t == max_spanish:\n",
    "            break\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, spanish_token_index[char]] = 1.\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, spanish_token_index[char]] = 1.\n",
    "    if i == max_pairs-1:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/6\n",
      "12000/12000 [==============================] - 65s 5ms/step - loss: 2.5933 - val_loss: 2.2855\n",
      "Epoch 2/6\n",
      "12000/12000 [==============================] - 63s 5ms/step - loss: 2.1065 - val_loss: 1.9738\n",
      "Epoch 3/6\n",
      "12000/12000 [==============================] - 62s 5ms/step - loss: 1.8755 - val_loss: 1.8111\n",
      "Epoch 4/6\n",
      "12000/12000 [==============================] - 60s 5ms/step - loss: 1.7421 - val_loss: 1.7119\n",
      "Epoch 5/6\n",
      "12000/12000 [==============================] - 60s 5ms/step - loss: 1.6335 - val_loss: 1.5995\n",
      "Epoch 6/6\n",
      "12000/12000 [==============================] - 61s 5ms/step - loss: 1.5425 - val_loss: 1.5143\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/keras/engine/network.py:877: UserWarning: Layer lstm_8 was passed non-serializable keyword arguments: {'initial_state': [<tf.Tensor 'lstm_7/while/Exit_2:0' shape=(?, 256) dtype=float32>, <tf.Tensor 'lstm_7/while/Exit_3:0' shape=(?, 256) dtype=float32>]}. They will not be included in the serialized model (and thus will be missing at deserialization time).\n",
      "  '. They will not be included '\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64\n",
    "epochs = 6\n",
    "\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(None, len(english_chars)))\n",
    "encoder = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder(encoder_inputs)\n",
    "# We discard `encoder_outputs` and only keep the states.\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Set up the decoder, using `encoder_states` as initial state.\n",
    "decoder_inputs = Input(shape=(None, len(spanish_chars)))\n",
    "# We set up our decoder to return full output sequences,\n",
    "# and to return internal states as well. We don't use the\n",
    "# return states in the training model, but we will use them in inference.\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = Dense(len(spanish_chars), activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define sampling models\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_outputs, state_h, state_c = decoder_lstm(decoder_inputs, initial_state=decoder_states_inputs)\n",
    "decoder_states = [state_h, state_c]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = Model([decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# ~~~~~ Sample with Temperature ~~~~~\n",
    "# ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "def sample(preds, temperature=1.0):\n",
    "\t# helper function to sample an index from a probability array\n",
    "\tpreds = np.asarray(preds).astype('float64')\n",
    "\tpreds = np.log(preds) / temperature\n",
    "\texp_preds = np.exp(preds)\n",
    "\tpreds = exp_preds / np.sum(exp_preds)\n",
    "\tprobas = np.random.multinomial(1, preds, 1)\n",
    "\treturn np.argmax(probas)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, len(spanish_chars)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, spanish_token_index[start_char]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "        \n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = spanish_index_tokens[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "        #print('sampled char is:', sampled_token_index, sampled_char)\n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == end_char or len(decoded_sentence) > max_spanish):\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, len(spanish_chars)))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Resumption of the session\n",
      "Decoded sentence: Es el prosento de la Comisión de la Comisión de la Comisión de la Comisión de la Comisión de la Comis\n",
      "Actual sentence: \tReanudación del período de sesiones\n",
      "\n",
      "-\n",
      "Input sentence: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Decoded sentence: En esta esta esta esta esta esta esta en el prober los de la Comisión de la Comisión de la Comisión d\n",
      "Actual sentence: \tDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n",
      "\n",
      "-\n",
      "Input sentence: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "Decoded sentence: Es el Presidento de la Comisión de la Comisión de la Comisión de la Comisión de la Comisión de la Com\n",
      "Actual sentence: \tComo todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n",
      "\n",
      "-\n",
      "Input sentence: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "Decoded sentence: Es el Presidento de la Comisión de la Comisión de la Comisión de la Comisión de la Comisión de la Com\n",
      "Actual sentence: \tSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n",
      "\n",
      "-\n",
      "Input sentence: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Decoded sentence: En esta esta esta esta esta esta esta en el prober los de la Comisión de la Comisión de la Comisión d\n",
      "Actual sentence: \tA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n",
      "\n",
      "-\n",
      "Input sentence: Please rise, then, for this minute' s silence.\n",
      "Decoded sentence: Es el Presidento de la Comisión de la Comisión de la Comisión de la Comisión de la Comisión de la Com\n",
      "Actual sentence: \tInvito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n",
      "\n",
      "-\n",
      "Input sentence: (The House rose and observed a minute' s silence)\n",
      "Decoded sentence: Señor Presidente, señor Presidente, señor Presidente, señor Presidente, señor Presidente, señor Presi\n",
      "Actual sentence: \t(El Parlamento, de pie, guarda un minuto de silencio)\n",
      "\n",
      "-\n",
      "Input sentence: Madam President, on a point of order.\n",
      "Decoded sentence: Señor Presidente, señor Presidente, señor Presidente, señor Presidente, señor Presidente, señor Presi\n",
      "Actual sentence: \tSeñora Presidenta, una cuestión de procedimiento.\n",
      "\n",
      "-\n",
      "Input sentence: You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "Decoded sentence: Es el Presidente, señor Presidente, señor Presidente, señor Presidente, señor Presidente, señor Presi\n",
      "Actual sentence: \tSabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\n",
      "\n",
      "-\n",
      "Input sentence: One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n",
      "Decoded sentence: En el mento de la Comisión de la Comisión de la Comisión de la Comisión de la Comisión de la Comisión\n",
      "Actual sentence: \tUna de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', english_lines[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Actual sentence:', spanish_lines[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention_3d_block(inputs, time_steps):\n",
    "    # inputs.shape = (batch_size, time_steps, input_dim)\n",
    "    input_dim = int(inputs.shape[2])\n",
    "    a = Permute((2, 1))(inputs)\n",
    "    a = Reshape((input_dim, time_steps))(a) # this line is not useful. It's just to know which dimension is what.\n",
    "    a = Dense(time_steps, activation='softmax')(a)\n",
    "    a_probs = Permute((2, 1), name='attention_vec')(a)\n",
    "    output_attention_mul = multiply([inputs, a_probs], name='attention_mul')\n",
    "    return output_attention_mul"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 100, 314)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 256)          584704      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_11 (RepeatVector) (None, 100, 256)     0           lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 256, 100)     0           repeat_vector_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 256, 100)     0           permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 256, 100)     10100       reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 100, 256)     0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "input_34 (InputLayer)           (None, 100, 334)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 100, 256)     0           repeat_vector_11[0][0]           \n",
      "                                                                 attention_vec[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "concat_spanish_and_attended (Co (None, 100, 590)     0           input_34[0][0]                   \n",
      "                                                                 attention_mul[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "lstm_22 (LSTM)                  (None, 100, 256)     867328      concat_spanish_and_attended[0][0]\n",
      "__________________________________________________________________________________________________\n",
      "dense_22 (Dense)                (None, 100, 334)     85838       lstm_22[0][0]                    \n",
      "==================================================================================================\n",
      "Total params: 1,547,970\n",
      "Trainable params: 1,547,970\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "latent_dim = 256\n",
    "batch_size = 64\n",
    "epochs = 96\n",
    "from keras.layers import RepeatVector, concatenate\n",
    "# Define an input sequence and process it.\n",
    "encoder_inputs = Input(shape=(max_english, len(english_chars)))\n",
    "encoder_out = LSTM(latent_dim)(encoder_inputs)\n",
    "encoder_repeat = RepeatVector(max_spanish)(encoder_out)\n",
    "attended_encoding = attention_3d_block(encoder_repeat, max_spanish)\n",
    "\n",
    "burn_in = Input(shape=(max_spanish, len(spanish_chars)))\n",
    "lstm_in = layers.concatenate([burn_in, attended_encoding], name='concat_spanish_and_attended')\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True)\n",
    "decoder_outputs = decoder_lstm(lstm_in)\n",
    "decoder_outputs = Dense(len(spanish_chars), activation='softmax')(decoder_outputs)\n",
    "\n",
    "# Define the model that will turn\n",
    "# `encoder_input_data` & `decoder_input_data` into `decoder_target_data`\n",
    "model = Model([encoder_inputs, burn_in], decoder_outputs)\n",
    "\n",
    "# Run training\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 12000 samples, validate on 3000 samples\n",
      "Epoch 1/96\n",
      "12000/12000 [==============================] - 83s 7ms/step - loss: 2.5907 - val_loss: 2.3498\n",
      "Epoch 2/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 2.1431 - val_loss: 1.9717\n",
      "Epoch 3/96\n",
      "12000/12000 [==============================] - 78s 6ms/step - loss: 1.8675 - val_loss: 1.7837\n",
      "Epoch 4/96\n",
      "12000/12000 [==============================] - 79s 7ms/step - loss: 1.7024 - val_loss: 1.6468\n",
      "Epoch 5/96\n",
      "12000/12000 [==============================] - 78s 6ms/step - loss: 1.5723 - val_loss: 1.5244\n",
      "Epoch 6/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 1.4676 - val_loss: 1.4438\n",
      "Epoch 7/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 1.3805 - val_loss: 1.3670\n",
      "Epoch 8/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 1.3089 - val_loss: 1.3054\n",
      "Epoch 9/96\n",
      "12000/12000 [==============================] - 80s 7ms/step - loss: 1.2486 - val_loss: 1.2621\n",
      "Epoch 10/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 1.1978 - val_loss: 1.2234\n",
      "Epoch 11/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 1.1552 - val_loss: 1.1807\n",
      "Epoch 12/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 1.1184 - val_loss: 1.1512\n",
      "Epoch 13/96\n",
      "12000/12000 [==============================] - 78s 7ms/step - loss: 1.0867 - val_loss: 1.1243\n",
      "Epoch 14/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 1.0596 - val_loss: 1.1108\n",
      "Epoch 15/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 1.0358 - val_loss: 1.0991\n",
      "Epoch 16/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 1.0150 - val_loss: 1.0792\n",
      "Epoch 17/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.9965 - val_loss: 1.0629\n",
      "Epoch 18/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.9798 - val_loss: 1.0539\n",
      "Epoch 19/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.9645 - val_loss: 1.0495\n",
      "Epoch 20/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.9511 - val_loss: 1.0354\n",
      "Epoch 21/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.9386 - val_loss: 1.0291\n",
      "Epoch 22/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.9270 - val_loss: 1.0219\n",
      "Epoch 23/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.9164 - val_loss: 1.0148\n",
      "Epoch 24/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.9066 - val_loss: 1.0084\n",
      "Epoch 25/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.8973 - val_loss: 1.0077\n",
      "Epoch 26/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8885 - val_loss: 1.0029\n",
      "Epoch 27/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8804 - val_loss: 0.9978\n",
      "Epoch 28/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8730 - val_loss: 0.9968\n",
      "Epoch 29/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8657 - val_loss: 0.9985\n",
      "Epoch 30/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.8588 - val_loss: 0.9959\n",
      "Epoch 31/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.8523 - val_loss: 0.9958\n",
      "Epoch 32/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.8459 - val_loss: 0.9922\n",
      "Epoch 33/96\n",
      "12000/12000 [==============================] - 78s 6ms/step - loss: 0.8401 - val_loss: 0.9878\n",
      "Epoch 34/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.8343 - val_loss: 0.9888\n",
      "Epoch 35/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.8289 - val_loss: 0.9909\n",
      "Epoch 36/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8237 - val_loss: 0.9863\n",
      "Epoch 37/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8183 - val_loss: 0.9902\n",
      "Epoch 38/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8137 - val_loss: 0.9880\n",
      "Epoch 39/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.8091 - val_loss: 0.9873\n",
      "Epoch 40/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8043 - val_loss: 0.9864\n",
      "Epoch 41/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.8000 - val_loss: 0.9923\n",
      "Epoch 42/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.7958 - val_loss: 0.9869\n",
      "Epoch 43/96\n",
      "12000/12000 [==============================] - 73s 6ms/step - loss: 0.7913 - val_loss: 0.9900\n",
      "Epoch 44/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.7874 - val_loss: 0.9906\n",
      "Epoch 45/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7834 - val_loss: 0.9935\n",
      "Epoch 46/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.7797 - val_loss: 0.9958\n",
      "Epoch 47/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7759 - val_loss: 0.9910\n",
      "Epoch 48/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.7720 - val_loss: 0.9913\n",
      "Epoch 49/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7686 - val_loss: 1.0018\n",
      "Epoch 50/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.7650 - val_loss: 0.9992\n",
      "Epoch 51/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.7617 - val_loss: 0.9946\n",
      "Epoch 52/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7584 - val_loss: 0.9990\n",
      "Epoch 53/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.7548 - val_loss: 1.0008\n",
      "Epoch 54/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.7517 - val_loss: 0.9994\n",
      "Epoch 55/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.7488 - val_loss: 1.0047\n",
      "Epoch 56/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.7458 - val_loss: 1.0121\n",
      "Epoch 57/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.7429 - val_loss: 1.0074\n",
      "Epoch 58/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.7398 - val_loss: 1.0139\n",
      "Epoch 59/96\n",
      "12000/12000 [==============================] - 72s 6ms/step - loss: 0.7370 - val_loss: 1.0097\n",
      "Epoch 60/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.7342 - val_loss: 1.0217\n",
      "Epoch 61/96\n",
      "12000/12000 [==============================] - 72s 6ms/step - loss: 0.7311 - val_loss: 1.0185\n",
      "Epoch 62/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.7287 - val_loss: 1.0216\n",
      "Epoch 63/96\n",
      "12000/12000 [==============================] - 72s 6ms/step - loss: 0.7260 - val_loss: 1.0137\n",
      "Epoch 64/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.7233 - val_loss: 1.0192\n",
      "Epoch 65/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.7206 - val_loss: 1.0248\n",
      "Epoch 66/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.7181 - val_loss: 1.0262\n",
      "Epoch 67/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7160 - val_loss: 1.0298\n",
      "Epoch 68/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.7136 - val_loss: 1.0369\n",
      "Epoch 69/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7108 - val_loss: 1.0318\n",
      "Epoch 70/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7084 - val_loss: 1.0378\n",
      "Epoch 71/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.7061 - val_loss: 1.0335\n",
      "Epoch 72/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.7043 - val_loss: 1.0548\n",
      "Epoch 73/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.7018 - val_loss: 1.0442\n",
      "Epoch 74/96\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.6998 - val_loss: 1.0486\n",
      "Epoch 75/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6974 - val_loss: 1.0461\n",
      "Epoch 76/96\n",
      "12000/12000 [==============================] - 71s 6ms/step - loss: 0.6953 - val_loss: 1.0538\n",
      "Epoch 77/96\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.6933 - val_loss: 1.0507\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 78/96\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.6914 - val_loss: 1.0543\n",
      "Epoch 79/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6895 - val_loss: 1.0536\n",
      "Epoch 80/96\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.6875 - val_loss: 1.0564\n",
      "Epoch 81/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6856 - val_loss: 1.0628\n",
      "Epoch 82/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6837 - val_loss: 1.0610\n",
      "Epoch 83/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6818 - val_loss: 1.0664\n",
      "Epoch 84/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6801 - val_loss: 1.0660\n",
      "Epoch 85/96\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.6780 - val_loss: 1.0652\n",
      "Epoch 86/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6765 - val_loss: 1.0740\n",
      "Epoch 87/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6750 - val_loss: 1.0713\n",
      "Epoch 88/96\n",
      "12000/12000 [==============================] - 69s 6ms/step - loss: 0.6731 - val_loss: 1.0788\n",
      "Epoch 89/96\n",
      "12000/12000 [==============================] - 70s 6ms/step - loss: 0.6714 - val_loss: 1.0863\n",
      "Epoch 90/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.6701 - val_loss: 1.0708\n",
      "Epoch 91/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.6677 - val_loss: 1.0775\n",
      "Epoch 92/96\n",
      "12000/12000 [==============================] - 76s 6ms/step - loss: 0.6667 - val_loss: 1.0831\n",
      "Epoch 93/96\n",
      "12000/12000 [==============================] - 77s 6ms/step - loss: 0.6649 - val_loss: 1.0854\n",
      "Epoch 94/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.6640 - val_loss: 1.0955\n",
      "Epoch 95/96\n",
      "12000/12000 [==============================] - 74s 6ms/step - loss: 0.6622 - val_loss: 1.0897\n",
      "Epoch 96/96\n",
      "12000/12000 [==============================] - 75s 6ms/step - loss: 0.6608 - val_loss: 1.0955\n"
     ]
    }
   ],
   "source": [
    "model.fit([encoder_input_data, decoder_input_data], \n",
    "          decoder_target_data, batch_size=batch_size, epochs=epochs, validation_split=0.2)\n",
    "# Save model\n",
    "model.save('s2s_attention_96.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_33 (InputLayer)           (None, 100, 314)     0                                            \n",
      "__________________________________________________________________________________________________\n",
      "lstm_21 (LSTM)                  (None, 256)          584704      input_33[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "repeat_vector_11 (RepeatVector) (None, 100, 256)     0           lstm_21[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "permute_11 (Permute)            (None, 256, 100)     0           repeat_vector_11[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "reshape_11 (Reshape)            (None, 256, 100)     0           permute_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "dense_21 (Dense)                (None, 256, 100)     10100       reshape_11[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "attention_vec (Permute)         (None, 100, 256)     0           dense_21[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "attention_mul (Multiply)        (None, 100, 256)     0           repeat_vector_11[0][0]           \n",
      "                                                                 attention_vec[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 594,804\n",
      "Trainable params: 594,804\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Define sampling models\n",
    "attn_encoder_model = Model(encoder_inputs, attended_encoding)\n",
    "attn_encoder_model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_attended_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    attended_np = attn_encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    burn_in_np = np.zeros((1, max_spanish, len(spanish_chars)))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    burn_in_np[0, 0, spanish_token_index[start_char]] = 1.\n",
    "\n",
    "    # Sampling loop for a batch of sequences (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    char_idx = 0\n",
    "    while not stop_condition:\n",
    "        char_idx += 1\n",
    "        output_tokens = model.predict([input_seq, burn_in_np])\n",
    "        # Sample a token\n",
    "        sampled_token_index =  sample(output_tokens[0, char_idx-1, :], 1.1)\n",
    "        sampled_char = spanish_index_tokens[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        burn_in_np[0, char_idx, sampled_token_index] = 1.\n",
    "        \n",
    "        \n",
    "        # Exit condition: either hit max length or find stop character.\n",
    "        if (sampled_char == end_char or len(decoded_sentence) > max_spanish) or char_idx == max_spanish-1:\n",
    "            stop_condition = True\n",
    "\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-\n",
      "Input sentence: Resumption of the session\n",
      "Decoded sentence: La competencia tiene la aniza entre los productores abrerán muy blegada los Estados miembros.\n",
      "\n",
      "Actual sentence: \tReanudación del período de sesiones\n",
      "\n",
      "-\n",
      "Input sentence: I declare resumed the session of the European Parliament adjourned on Friday 17 December 1999, and I would like once again to wish you a happy new year in the hope that you enjoyed a pleasant festive period.\n",
      "Decoded sentence: Esto recorda muy sustancialidad del Convenio debe considerarse vivilantes de la formulidad de agua.\n",
      "Actual sentence: \tDeclaro reanudado el período de sesiones del Parlamento Europeo, interrumpido el viernes 17 de diciembre pasado, y reitero a Sus Señorías mi deseo de que hayan tenido unas buenas vacaciones.\n",
      "\n",
      "-\n",
      "Input sentence: Although, as you will have seen, the dreaded 'millennium bug' failed to materialise, still the people in a number of countries suffered a series of natural disasters that truly were dreadful.\n",
      "Decoded sentence: Entrego todasoval concreta del Reins manoras -y pareciendos escandúan conflemosa, pero que, en 1998\n",
      "Actual sentence: \tComo todos han podido comprobar, el gran \"efecto del año 2000\" no se ha producido. En cambio, los ciudadanos de varios de nuestros países han sido víctimas de catástrofes naturales verdaderamente terribles.\n",
      "\n",
      "-\n",
      "Input sentence: You have requested a debate on this subject in the course of the next few days, during this part-session.\n",
      "Decoded sentence: Necesitamos una vícula que presentará respecto a novie, pediraría hacer que el acuerdo a la adhesió\n",
      "Actual sentence: \tSus Señorías han solicitado un debate sobre el tema para los próximos días, en el curso de este período de sesiones.\n",
      "\n",
      "-\n",
      "Input sentence: In the meantime, I should like to observe a minute' s silence, as a number of Members have requested, on behalf of all the victims concerned, particularly those of the terrible storms, in the various countries of the European Union.\n",
      "Decoded sentence: Aunque esta innovación fundamenta)\n",
      "\n",
      "Actual sentence: \tA la espera de que se produzca, de acuerdo con muchos colegas que me lo han pedido, pido que hagamos un minuto de silencio en memoria de todas las víctimas de las tormentas, en los distintos países de la Unión Europea afectados.\n",
      "\n",
      "-\n",
      "Input sentence: Please rise, then, for this minute' s silence.\n",
      "Decoded sentence: Me gustaría experir su condicionabilidad de los trabajadores cantidades.\n",
      "\n",
      "Actual sentence: \tInvito a todos a que nos pongamos de pie para guardar un minuto de silencio.\n",
      "\n",
      "-\n",
      "Input sentence: (The House rose and observed a minute' s silence)\n",
      "Decoded sentence: Me parece excelente, el Sr. Seixues, puede crear unas poblaciones que todos apostanos por la Comisi\n",
      "Actual sentence: \t(El Parlamento, de pie, guarda un minuto de silencio)\n",
      "\n",
      "-\n",
      "Input sentence: Madam President, on a point of order.\n",
      "Decoded sentence: Hago que trayer presente el principio de precaución del Gobierno alcance, estudimo que, al menos an\n",
      "Actual sentence: \tSeñora Presidenta, una cuestión de procedimiento.\n",
      "\n",
      "-\n",
      "Input sentence: You will be aware from the press and television that there have been a number of bomb explosions and killings in Sri Lanka.\n",
      "Decoded sentence: Todo el mundo está de ahora al tema de la formulación de impuesto sobre los Estados miembros del pr\n",
      "Actual sentence: \tSabrá usted por la prensa y la televisión que se han producido una serie de explosiones y asesinatos en Sri Lanka.\n",
      "\n",
      "-\n",
      "Input sentence: One of the people assassinated very recently in Sri Lanka was Mr Kumar Ponnambalam, who had visited the European Parliament just a few months ago.\n",
      "Decoded sentence: He escuchado todosenos, afectamos, sobre los valores \" contectos, con cada petición de los marcasor\n",
      "Actual sentence: \tUna de las personas que recientemente han asesinado en Sri Lanka ha sido al Sr. Kumar Ponnambalam, quien hace pocos meses visitó el Parlamento Europeo.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(10):\n",
    "    # Take one sequence (part of the training set)\n",
    "    # for trying out decoding.\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_attended_sequence(input_seq)\n",
    "    print('-')\n",
    "    print('Input sentence:', english_lines[seq_index])\n",
    "    print('Decoded sentence:', decoded_sentence)\n",
    "    print('Actual sentence:', spanish_lines[seq_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
