{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv\n",
    "from collections import Counter, defaultdict\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! awk -F \"\\t\" '{print $2,$5,$6,$7,$8,$9,$10,$11,$13,$14,$15,$16,$17,$18,$19,$20,$21,$23,$24,$25,$26}' OFS='\\t' /mnt/ml4cvd/projects/jamesp/data/returned_lv_mass.tsv > /home/sam/mri_continuous_5k.tsv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maps = defaultdict(list)\n",
    "with open('/home/sam/mri_continuous_5k.tsv', 'r') as volumes:\n",
    "    lol = list(csv.reader(volumes, delimiter='\\t'))\n",
    "    fields = lol[0][1:]  # Assumes sample id is the first field\n",
    "    print(f\"CSV of floats header:{fields}\")\n",
    "    for row in lol[1:]:\n",
    "        sample_id = row[0]\n",
    "        for i in range(len(fields)):\n",
    "            try:\n",
    "                data_maps[fields[i]].append(float(row[i+1]))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "for f in data_maps:\n",
    "    a = np.array(data_maps[f])\n",
    "    print(f\"TMAPS['{f}'] = TensorMap('{f}',  group='continuous', normalization={{'mean': {np.mean(a)}, 'std': {np.std(a)} }}, loss='logcosh', channel_map={{'{f}': 0}})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maps = defaultdict(list)\n",
    "with open('/home/sam/med_phenotype_categorical.csv', 'r') as volumes:\n",
    "    lol = list(csv.reader(volumes, delimiter=','))\n",
    "    fields = lol[0][1:]  # Assumes sample id is the first field\n",
    "    print(f\"CSV header:{fields}\")\n",
    "    for row in lol[1:]:\n",
    "        sample_id = row[0]\n",
    "        for i in range(len(fields)):\n",
    "            try:\n",
    "                data_maps[fields[i]].append(float(row[i+1]))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "for f in data_maps:\n",
    "    a = np.array(data_maps[f])\n",
    "    r = np.sum(a) / a.shape[0]\n",
    "    cm = f\"{{'no_{f}': 0, '{f}': 1}}\"\n",
    "    print(f\"TMAPS['{f}'] = TensorMap('{f}', group='categorical_index', channel_map={cm}, loss_weight=100.0, loss=weighted_crossentropy([{r:0.4f}, {1-r:0.4f}], '{f}'))\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clbrain = ''\n",
    "with open('/home/sam/brain_fields.csv', 'r') as volumes:\n",
    "    lol = list(csv.reader(volumes, delimiter=','))\n",
    "    for row in lol[1:]:\n",
    "        if int(row[0]) < 26000:\n",
    "            clbrain += '('+row[0]+'), '\n",
    "print(clbrain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_maps = defaultdict(list)\n",
    "with open('/home/sam/ukb7089_instance2_bsa.csv', 'r') as volumes:\n",
    "    lol = list(csv.reader(volumes, delimiter=','))\n",
    "    fields = lol[0][1:]  # Assumes sample id is the first field\n",
    "    print(f\"CSV of floats header:{fields}\")\n",
    "    for row in lol[1:]:\n",
    "        sample_id = row[0]\n",
    "        for i in range(len(fields)):\n",
    "            try:\n",
    "                data_maps[fields[i]].append(float(row[i+1]))\n",
    "            except:\n",
    "                pass\n",
    "            \n",
    "for f in data_maps:\n",
    "    a = np.array(data_maps[f])\n",
    "    print(f'min: {np.min(a)}  max: {np.max(a)}')\n",
    "    print(f\"TMAPS['{f}'] = TensorMap('{f}',  group='continuous', normalization={{'mean': {np.mean(a)}, 'std': {np.std(a)} }}, loss='logcosh', channel_map={{'{f}': 0}})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = Counter()\n",
    "duped = Counter()\n",
    "with open('/home/sam/all_ecg_for_labeling.tsv', 'r') as volumes:\n",
    "    lol = list(csv.reader(volumes, delimiter='\\t'))\n",
    "    fields = lol[0]\n",
    "    print(f\"CSV of labels header:{fields}\")\n",
    "    for row in lol[1:]:\n",
    "        sample_id = row[0]\n",
    "        for i in range(3, len(fields)):\n",
    "            split_labels = row[i].split(',')\n",
    "            for label in split_labels:\n",
    "                if label != '':\n",
    "                    stats[f'{fields[i]}___{label.strip()}'] += 1\n",
    "                if len(split_labels) > 1:\n",
    "                    for l2 in split_labels:\n",
    "                        if l2 != label:\n",
    "                            duped[f'{fields[i]}_{label}___{l2.strip()}'] += 1\n",
    "                \n",
    "for k,v in sorted(stats.items(), key=lambda x: (x[0].split('___')[0],-x[1])):\n",
    "    print(f'{k} has: {stats[k]}')\n",
    "    \n",
    "for k,v in sorted(duped.items(), key=lambda x: (x[0].split('___')[0],-x[1])):\n",
    "    print(f'{k.split(\"___\")[0]} co-occurs with {k.split(\"___\")[1]} {duped[k]} times')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reader = csv.reader(open(\"/home/sam/ml/trained_models/ecg_rr_autoencode_lvmass_bn/inference_ecg_rr_autoencode_lvmass_bn.tsv\"), delimiter=\"\\t\")\n",
    "sortedlist = sorted(reader, key=lambda row: -9e9 if row[0] =='sample_id' else float(row[3]), reverse=False)\n",
    "for i in range(15):\n",
    "    print(f'https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/{sortedlist[i][0]}.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/2903227.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/1802628.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/5071800.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/2764566.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/2519267.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/1668411.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/2718173.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/5307336.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/5083023.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/1682170.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/5835419.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/5051083.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/4747639.pdf\n",
    "https://console.cloud.google.com/storage/browser/_details/ml4h/ecg_views_9_7_2019_all/2384220.pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
