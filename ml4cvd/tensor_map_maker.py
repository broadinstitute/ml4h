import os
import logging
import numpy as np
import pandas as pd
from typing import List
from typing.io import TextIO

from defines import MRI_ZOOM_INPUT, MRI_ZOOM_MASK, TENSOR_MAPS_FILE_NAME, MRI_SEGMENTED_CHANNEL_MAP
from tensor_writer_ukbb import disease_prevalence_status, get_disease2tsv, disease_incidence_status, disease_censor_status


def write_tensor_maps(args) -> None:
    logging.info("Making tensor maps...")

    tensor_maps_file = f"{args.output_folder}/{TENSOR_MAPS_FILE_NAME}.py"
    with open(tensor_maps_file, 'w') as f:
        f.write(_get_tensor_map_file_imports())
        _write_dynamic_mri_tensor_maps(args.x, args.y, args.z, args.zoom_width, args.zoom_height, args.label_weights, args.t, f)
        # Commented out until BigQuery version is implemented
        #_write_megans_tensor_maps(f)
        _write_disease_tensor_maps(args.phenos_folder, f)
        _write_disease_tensor_maps_time(args.phenos_folder, f)
        _write_disease_tensor_maps_incident_prevalent(args.phenos_folder, f)
        f.write('\n')
        logging.info(f"Wrote the tensor maps to {tensor_maps_file}.")


def _get_tensor_map_file_imports() -> str:
    return f"#  TensorMaps automatically generated by {os.path.basename(__file__)}\n" \
           f"#  DO NOT EDIT\n\n" \
           f"from TensorMap import TensorMap\n" \
           f"from tensor_maps_by_hand import TMAPS \n" \
           f"from metrics import weighted_crossentropy\n" \
           f"from defines import MRI_SEGMENTED_CHANNEL_MAP\n\n\n"


def _write_dynamic_mri_tensor_maps(x: int, y: int, z: int, zoom_width: int, zoom_height: int, label_weights: List[int],
                                   t: int, f: TextIO) -> None:
    """Write TensorMappings that are adjustable from the command line to the filepath (f) provided.

    Arguments:
        :param x: MRI x dimension
        :param y: MRI y dimension
        :param z: MRI z dimension
        :param zoom_width: MRI zoomed in x dimension size
        :param zoom_height: MRI zoomed in y dimension size
        :param label_weights: Class weights for weighted cross entropy to counter class imbalance problems
        :param t: MRI time slices
        :param f: tensor_maps_by_script.py file path
    """
    if label_weights is None:
        label_weights = [20.0, 250.0, 250.0]
    f.write(f"TMAPS['mri-xyt-lax'] = TensorMap('mri-xyt-lax', ({x}, {y}, {z}), "
            f"dependent_map=TMAPS['lax-view-detect'])\n")
    f.write(f"TMAPS['mri-xyt-lax'] = TensorMap('mri-xyt-sax', ({x}, {y}, {z}), "
            f"dependent_map=TMAPS['sax-view-detect'])\n")
    f.write(f"TMAPS['mri-xyt-lax'] = TensorMap('mri-xyt-slax', ({x}, {y}, {z}), "
            f"dependent_map=TMAPS['slax-view-detect'])\n")
    f.write(f"TMAPS['{_segmented_map('mri_slice')}'] = TensorMap('{_segmented_map('mri_slice')}', "
            f"({x}, {y}, {len(MRI_SEGMENTED_CHANNEL_MAP)}), loss='categorical_crossentropy', "
            f"group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP)\n")
    f.write(f"TMAPS['{_segmented_map('mri_slice')}_weighted'] = TensorMap('{_segmented_map('mri_slice')}', ({x}, "
            f"{y}, {len(MRI_SEGMENTED_CHANNEL_MAP)}), group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP,"
            f"loss=weighted_crossentropy({label_weights}, '{_segmented_map('mri_slice')}'))\n")
    f.write(f"TMAPS['mri_slice'] = TensorMap('mri_slice', ({x}, {y}, 1), "
            f"dependent_map=TMAPS['{_segmented_map('mri_slice')}'])\n")
    f.write(f"TMAPS['mri_slice_weighted'] = TensorMap('mri_slice', ({x}, {y}, 1), "
            f"dependent_map=TMAPS['{_segmented_map('mri_slice')}_weighted'])\n")
    f.write(f"TMAPS['{_segmented_map('cine_segmented_sax_inlinevf')}'] = "
            f"TensorMap('{_segmented_map('cine_segmented_sax_inlinevf')}', ({x}, {y}, {t}, "
            f"{len(MRI_SEGMENTED_CHANNEL_MAP)}), loss='categorical_crossentropy', group='categorical', "
            f"channel_map=MRI_SEGMENTED_CHANNEL_MAP)\n")
    f.write(f"TMAPS['{_segmented_map('cine_segmented_sax_inlinevf')}_weighted'] = "
            f"TensorMap('{_segmented_map('cine_segmented_sax_inlinevf')}', ({x}, {y}, {t}, "
            f"{len(MRI_SEGMENTED_CHANNEL_MAP)}), group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP, "
            f"loss=weighted_crossentropy({label_weights}, '{_segmented_map('cine_segmented_sax_inlinevf')}'))\n")
    f.write(f"TMAPS['cine_segmented_sax_inlinevf'] = TensorMap('cine_segmented_sax_inlinevf', ({x}, {y}, "
            f"{t}, 1), dependent_map=TMAPS['{_segmented_map('cine_segmented_sax_inlinevf')}'])\n")
    f.write(f"TMAPS['cine_segmented_sax_inlinevf_weighted'] = TensorMap('cine_segmented_sax_inlinevf', "
            f"({x}, {y}, {t}, 1), "
            f"dependent_map=TMAPS['{_segmented_map('cine_segmented_sax_inlinevf')}_weighted'])\n")
    f.write(f"TMAPS['{MRI_ZOOM_MASK}'] = TensorMap('{MRI_ZOOM_MASK}', ({zoom_width},{zoom_height},{t}, "
            f"{len(MRI_SEGMENTED_CHANNEL_MAP)}), loss='categorical_crossentropy', group='categorical', "
            f"channel_map=MRI_SEGMENTED_CHANNEL_MAP)\n")
    f.write(f"TMAPS['{MRI_ZOOM_MASK}_weighted'] = TensorMap('{MRI_ZOOM_MASK}', ({zoom_width}, {zoom_height}, "
            f"{t}, {len(MRI_SEGMENTED_CHANNEL_MAP)}), group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP,"
            f"loss=weighted_crossentropy({label_weights}, '{MRI_ZOOM_MASK}'))\n")
    f.write(f"TMAPS['{MRI_ZOOM_INPUT}'] = TensorMap('{MRI_ZOOM_INPUT}',({zoom_width},{zoom_height},{t}, "
            f"1), dependent_map=TMAPS['{MRI_ZOOM_MASK}'])\n")
    f.write(f"TMAPS['{MRI_ZOOM_INPUT}_weighted'] = TensorMap('{MRI_ZOOM_INPUT}',({zoom_width},{zoom_height},{t}, "
            f"1), dependent_map=TMAPS['{MRI_ZOOM_MASK}_weighted'])\n")
    f.write(f"TMAPS['{_segmented_map('mri_systole_diastole')}']=TensorMap('{_segmented_map('mri_systole_diastole')}', "
            f"({x}, {y}, 2, {len(MRI_SEGMENTED_CHANNEL_MAP)}), loss='categorical_crossentropy', "
            f"group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP)\n")
    f.write(f"TMAPS['{_segmented_map('mri_systole_diastole')}_weighted'] = TensorMap("
            f"'{_segmented_map('mri_systole_diastole')}', ({x}, {y}, 2, {len(MRI_SEGMENTED_CHANNEL_MAP)}), "
            f"group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP,"
            f"loss=weighted_crossentropy({label_weights}, '{_segmented_map('mri_systole_diastole')}'))\n")
    f.write(f"TMAPS['mri_systole_diastole'] = TensorMap('mri_systole_diastole', ({x}, {y}, 2, 1), "
            f"dependent_map=TMAPS['{_segmented_map('mri_systole_diastole')}'])\n")
    f.write(f"TMAPS['mri_systole_diastole_weighted'] = TensorMap('mri_systole_diastole', ({x}, {y}, 2, 1), "
            f"dependent_map=TMAPS['{_segmented_map('mri_systole_diastole')}_weighted'])\n")
    f.write(f"TMAPS['{_segmented_map('mri_systole_diastole_8')}'] = TensorMap('"
            f"{_segmented_map('mri_systole_diastole_8')}', ({x}, {y}, 8, {len(MRI_SEGMENTED_CHANNEL_MAP)}), "
            f"loss='categorical_crossentropy',  group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP)\n")
    f.write(f"TMAPS['{_segmented_map('mri_systole_diastole_8')}_weighted'] = TensorMap("
            f"'{_segmented_map('mri_systole_diastole_8')}', ({x}, {y}, 8, {len(MRI_SEGMENTED_CHANNEL_MAP)}), "
            f"group='categorical', channel_map=MRI_SEGMENTED_CHANNEL_MAP,"
            f"loss=weighted_crossentropy({label_weights}, '{_segmented_map('mri_systole_diastole_8')}'))\n")
    f.write(f"TMAPS['mri_systole_diastole_8'] = TensorMap('mri_systole_diastole_8', ({x}, {y}, 8, 1), "
            f"dependent_map=TMAPS['{_segmented_map('mri_systole_diastole_8')}'])\n\n")


def _write_disease_tensor_maps(phenos_folder: str, f: TextIO)-> None:
    f.write(f"\n\n#  TensorMaps for MPG disease phenotypes\n")
    disease2tsv = get_disease2tsv(phenos_folder)
    status = disease_censor_status(disease2tsv)
    for d in sorted(list(disease2tsv.keys())):
        total = len(status[d])
        diseased = np.sum(list(status[d].values()))
        factor = int(total / (diseased * 2))
        f.write(f"TMAPS['{d}'] = TensorMap('{d}', group = 'categorical_index', channel_map = {{'no_{d}':0, '{d}':1}}, "
                f"loss = weighted_crossentropy([1.0, {factor}], '{d}'))\n")


def _write_disease_tensor_maps_incident_prevalent(phenos_folder: str, f: TextIO) -> None:
    f.write(f"\n\n#  TensorMaps for prevalent and incident MPG disease phenotypes\n")
    disease2tsv = get_disease2tsv(phenos_folder)
    status_p = disease_prevalence_status(disease2tsv, 1000000, 2000000)
    status_i = disease_incidence_status(disease2tsv, 1000000, 2000000)
    for disease in sorted(list(disease2tsv.keys())):
        total = len(status_p[disease])
        diseased_p = np.sum(list(status_p[disease].values()))
        factor_p = int(total / (1 + (diseased_p * 3)))
        diseased_i = np.sum(list(status_i[disease].values()))
        factor_i = int(total / (1 + (diseased_i * 3)))
        f.write(f"TMAPS['{disease}_prevalent_incident'] = TensorMap('{disease}', group='categorical_date', "
                f"channel_map={{'no_{disease}':0, 'prevalent_{disease}':1, 'incident_{disease}':2}}, "
                f"loss=weighted_crossentropy([1.0, {factor_p}, {factor_i}], '{disease}_prevalent_incident'))\n")


def _write_disease_tensor_maps_time(phenos_folder: str, f: TextIO) -> None:
    f.write(f"\n\n#  TensorMaps for date regression on MPG disease phenotypes\n")
    disease2tsv = get_disease2tsv(phenos_folder)
    for d in sorted(list(disease2tsv.keys())):
        f.write(f"TMAPS['{d}_time']=TensorMap('{d}',group='diagnosis_time',channel_map={{'{d}_time':0}},loss='mse')\n")

            
def _write_megans_tensor_maps(f: TextIO):
    annotation_units = 2
    count = 0

    pyukbb_data_path = '/mnt/disks/data/raw/pyukbb_data/'
    to_be_exported_csv_path = '/mnt/disks/data/raw/pyukbb_data/mb-ukbb-selected-fields.csv'
    available_fields_pd = pd.read_csv(to_be_exported_csv_path)
    fields = _get_all_available_fields(available_fields_pd)

    continuous_field_ids = fields.loc[fields['ValueType']=='Continuous']['FieldID']
    integer_field_ids = fields.loc[fields['ValueType']=='Integer']['FieldID']
    f.write(f"\n\n#  Continuous tensor maps from pyukbb\n")
    for field_id in continuous_field_ids.append(integer_field_ids):
        print(field_id)
        group = 'continuous'
        pf = pyukbb.UKBioBankParsedField.from_file(_get_pkl_path_for_field(field_id, pyukbb_data_path))
        name = str(field_id) + "_" + pf.field.replace("-", "").replace(" ", "-").replace("(", "").replace(")", "")
        name = name.replace("'", "").replace(",", "").replace("/", "").replace("+", "") + "_0_0"
        try:
            tensor = pyukbb.utils.get_dense_tensor_for_sample_ids(pf, list(pf.included_ukbb_sample_ids))
        except IndexError:
            print(name + " could not be tensorized.")
            continue

        if pf.has_coding:
            # group = 'continuous_with_categorical'
            print("pf has coding")

        df = pd.DataFrame({'sample': list(pf.included_ukbb_sample_ids)})
        for k, v in pf.category_coding_map.items():
            df[v] = tensor[:, 0, 0, k]
        if 'Do not know' in df.columns and 'Prefer not to answer' in df.columns:
            df['all_missing'] = df['Do not know'] + df['Prefer not to answer'] + df[
                'Not available in UKBB database']
        # -313 is "ongoing"
        elif 'Ongoing when data entered' in df.columns:
            df['all_missing'] = df['Ongoing when data entered'] + df['Not available in UKBB database']
        else:
            df['all_missing'] = df['Not available in UKBB database']

        if 'Less than one' in df.columns:
            df['true_value'] = df['Less than one'].apply(lambda x: .5 if x == 1 else 0)
            df['true_value'] = df['true_value'] + df['Value']
        else:
            df['true_value'] = df['Value']

        mean = np.mean(df.loc[df['all_missing'] == 0]['true_value'])
        std = np.std(df.loc[df['all_missing'] == 0]['true_value'])

        if mean is np.nan:
            logging.warning(name + " had nans")
            continue

        f.write(f"TMAPS['{field_id}_0'] = TensorMap('{name}', group='{group}', channel_map={{'{name}': 0, "
                f"'not-missing': 1}}, normalization={{'mean': {mean}, 'std': {std}}}, "
                f"annotation_units={annotation_units})\n")
        count += 1


def _segmented_map(name):
    return name + '_segmented'


def _get_pkl_path_for_field(field_id: int, pyukbb_data_path: str):
    """Returns the path to the .pkl file that contained `UKBioBankParsedField` for a given FieldID."""
    for _, _, files in os.walk(pyukbb_data_path):
        for file in files:
            if file.find(f'FieldID_{field_id}.pkl') > -1:
                return os.path.join(pyukbb_data_path, file)
    raise FileNotFoundError('Cannot find pyukbb .pkl file for field ID {field_id}!')


def _get_all_available_fields(available_fields_pd, keyword: str = None, category: int = None):
    filtered = available_fields_pd
    if category is not None:
        filtered = filtered[filtered.Category == category]
    if keyword is not None:
        filtered = filtered[filtered.Field.str.contains(keyword, case=False)]
    return filtered
